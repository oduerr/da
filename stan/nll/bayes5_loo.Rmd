---
output: 
  md_document: 
    variant: markdown_github
  pdf_document:
    toc: true
    toc_depth: 2
---

**Currently under construction**

## Understanding NLL and LOO
Some notes for the evaluation of the performance of a probabilistic prediction. All valid and applicable measures are variants of the negative log likelihood NLL. In the Bayesian context often a quantity known under the name log predictive probability `lppd` is used. In the regression setting, the question remains at which points $x_i$ this quantity shall be evaluated. In https://arxiv.org/abs/1507.04544 the quantity `elpd` the *expected log pointwise predictive density for a new dataset* is thus defined as:

$$
  \mathit{elpd} = \sum_{i=1}^{N} E_{y \sim p(y|x_i)}\left[\log(p(y|x_i,D))\right]  
$$
This quantity requires integrating over the data generating process $p(y|x_i)$ for a given $x_i$. The first approximation which can be done, is to evaluate the expectation at a test set at the single points $y_i$.

$$
  \mathit{elpd.test} = \hat{\mathit{elpd}}= \sum_{i=1}^{N} \log(p(y_i|x_i,D)) = -N \cdot \mathit{NLL}
$$
It is thus $N$ times the usual NLL.

### Random Data
Some Data for linear regression:
```{r}
N = 50
SD = 0.3
A = 1
B = 2
gen_data = function(){
  x = seq(-2,2,length.out = N)
  y = rnorm(N,A*x+B, sd=SD)  
  return (data.frame(x=x,y=y))
}
set.seed(1)
dat = gen_data()
x=dat$x
y=dat$y
plot(x,y)
```


### Theoretical value
In principle we know that the data is distributed normally with constant spread SD. In this case the elpd would be.

$$
\mathit{elpd}=N \cdot \int_{-\infty}^{+\infty} \log(N(y|0,0.3)) \cdot N(y|0, 0.3) \; dy
$$

```{r, message=FALSE, warning=FALSE,results='hide'}
  # Numerical Integration
  f = function(x) dnorm(x, sd=SD)*log(dnorm(x, sd=SD))
  integrate(f, -1,1)
  integrate(f, -2,2)
  integrate(f, -4,4)
  (elpd.theo = N*integrate(f, -4,4)$value)
  (nll.theo = -integrate(f, -4,4)$value)
  #integrate(f, -Inf,Inf) not working
  
  # Sampling
  d = rnorm(1e6, mean=0,sd=0.3)
  (elpd.samp = N*mean(log(dnorm(d, mean=0, sd=0.3))))
```

### Taking spread of y into account 

```{r, eval=FALSE, echo=FALSE}
  vals = rep(NA, 1000)
  set.seed(1)
  for (i in 1:length(vals)) {
    dat = gen_data()
    vals[i] = -mean(log(dnorm(dat$y,A*dat$x+B, sd=SD)))
  }
  plot(density(vals), xlab="NLL", main='Distribution of the NLL estimates')
  abline(v = nll.theo, col='green')
  abline(v = mean(vals), col='red')
```

Caption: 'The NLL evaluated from sampling of the data generating process. Note there is quite spread '

### Fitting with stan
a) Fit the and check the results
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE,results='hide', cache = TRUE}
  library(rstan)
  fit = stan(file = 'lr.stan', data=list(N=length(y),y=y,x=x))
  print(fit, pars = c('a','b','sigma'))
```


### Estimating the elpd

The package `loo` provides advances estimates of the `elpd`. For that a matrix of the log-likelihood needs to be provided. The dimensions are: S by N, where S is the size of the posterior sample (with all chains merged) and N is the number of data points. We caluculate the log-likelihood `ps` as follows.


```{r}
  library(loo)
  calc_ps = function () {
    a_sam = rstan::extract(fit, 'a')[[1]]
    b_sam = rstan::extract(fit, 'b')[[1]]
    s_sam = rstan::extract(fit, 'sigma')[[1]]
    samples = length(a_sam)
    ps = matrix(NA, nrow = samples, ncol=N)
    for (i in (1:samples)){
      for (j in (1:length(x))){
        ps[i,j] = dnorm(y[j],a_sam[i] * x[j] + b_sam[i], s_sam[i], log=TRUE)
      }
    }
    return (ps)
  }
  ps = calc_ps()
```
Creating of summary statistics.
```{r}
  make_df = function(ps){
    (elpd.train = N*mean(ps)) 
    res = loo::loo(ps)
    (elpd.loo = res$elpd_loo)
    (p.loo = res$estimates['p_loo',1])
    res = loo::waic(ps)$estimates
    (elpd.waic = res['elpd_waic',1])
    (p.waic = res['p_waic',1])
    
    df = data.frame(
        loo = c(elpd.loo, -elpd.loo/N, p.loo),
        waic = c(elpd.waic, -elpd.waic/N, p.waic),
        training = c(elpd.train, -elpd.train/N, NA)
        )
    return(df)
  }
  ps = calc_ps()
  df = make_df(ps)
  row.names(df) = c('elpd', 'NLL', 'p')
  print(df)
  elpd.theo
  library("kableExtra")
  kableExtra::kable(df)
```
  
### Comparison with example from statistical rethinking

In the book (page 222, R Code 7.19) they use the following example:
```{r}
  x = cars$speed
  y = cars$dist
  N = length(y)
  #x =  c(-2.,-0.66666, 0.666, 2.)
  #y = c(-6.25027354, -2.50213382, -6.07525495,  7.92081243)
  plot(x,y)
```

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE,results='hide', cache = TRUE}
library(rstan)
fit = stan(file = 'lr.stan', data=list(N=length(y),y=y,x=x))
print(fit, pars = c('a','b','sigma'))
```

```{r}
  ps = calc_ps()
  df = make_df(ps)
  row.names(df) = c('elpd', 'NLL', 'p')
  library("kableExtra")
  kableExtra::kable(df)
```

### Manual calculation of the WAIC

The elpd estimate can be calculated by summing up the posterior variances of the different MCMC samples.

```{r}
  (p.waic.manual = sum(apply(ps, 2, var))) #p.waid
  elpd.train = df['elpd','training']
  elpd.train -  p.waic.manual #
  -2*(elpd.train -  p.waic.manual) #422.85 in book 423.
```

Discussion: The effective number of parameters, is like in the `loo` routine. However, the WAIC is probably calculated slightly different in `loo` compared to the approach in statistical rethinking and done in the manual approach.



