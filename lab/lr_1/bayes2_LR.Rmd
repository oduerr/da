---
output:
  pdf_document:
    highlight: pygments
    includes:
      in_header: header_lsg.tex
---

```{r, echo=FALSE, eval=TRUE, message=FALSE}
  if (exists("lsg") == FALSE){ #Nur falls von RStudio aufgerufen
    lsg <- TRUE   #Wenn man die lsg ausgeben will dann lsg <- TRUE sonst 
  }
  if (exists("baseDir") == FALSE){ #Nur falls von RStudio aufgerufen
    baseDir = getwd()
  }
  ig <- function(name, scale = 1) {
     return (paste0("\\includegraphics[ scale =", scale,"]{", baseDir,"/", name, "}"))
  }
```

## Linear Regression

In this exercise, we will compare the Maximum Likelihood (ML) solution with a Bayesian solution for linear regression. We will generate synthetic data, fit models using both methods, and analyze the results to understand the differences in their approaches and outcomes.

a)  Use the same data generating process as in `Maximum Likelihood Principle (linear regression)`. Simulate 50 data points $(x,y)$ under the assumptions from linear regression $y \sim Norm(\mu=2.0 \cdot x + 1.0, \sigma=1)$. Let $x$ be in the range from -2 to 2. Hint: use the function $rnorm$.

```{r, echo=lsg, eval=lsg}
  set.seed(1)
  N = 50
  x = seq(-2,2, length.out = N)
  y = rnorm(N, mean = 2 * x + 1, sd = 1)
```

b)  Fit the maximum likelihood solution with the R command `lm`. Plot the Data and $\mu_{ML} = E(y|x)$

```{r, echo=lsg, eval=lsg, warning=FALSE}
  maxl = lm(y ~ x)
  d = coef(maxl)
  a = d[2]
  b = d[1]
  curve(a*x+b, from = -5, to=5)
  points(x,y)
  a
  b
```

c)  Do a Bayesian analysis with a extremely flat prio, e.g. $a,b \sim unif(-100,100)$ and $\sigma \sim uniform(0,100)$. Extract samples of $a,b$ from the posterior and plot them. Compare the mean of the Bayesian Analysis with the Maximum Likelihood Solution.

#### Hints:
Stan: In case you are stuck, the stan-file can be found at <https://github.com/oduerr/da/blob/master/lab/lr_1/lr.stan>

Creating samples from posterior:
```{r, echo=TRUE, eval=FALSE}
  library(cmdstanr)
  m_rcmdstan <- cmdstan_model('Link_to_stan_file.stan')
  post = m_rcmdstan$sample(data=list(N=N,x=x, y=y)) #Sampling from the posterior
  samples = post$draws(format = "df") #Extracting the samples in standard data.frame format
```

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
library(cmdstanr)
m_rcmdstan <- cmdstan_model('~/Documents/GitHub/da/lab/lr_1/lr.stan')

#model = stan_model(model_code = model_code)
#post = stan(model_code = model_code, data=list(N=N,x=x, y=y))
post = m_rcmdstan$sample(data=list(N=N,x=x, y=y))
#samples = extract(post)
samples = post$draws(format = "df")

#In nice
library(ggplot2)
ggplot(samples,aes(x=a,y=b)) + 
  geom_point(size=0.1, alpha=0.2) + 
  geom_density2d() 

#Also ok (non-ggplot)
# plot(samples$a,samples$b, pch='.')
a
mean(samples$a) 
hist(samples$a)
plot(density(samples$a))

b
mean(samples$b)
lm(y ~ x)
```

d)  At $x=1$ calculate $\mu_|x = a+bx$ from the posterior samples and plot it. Extract the mean and the $0.05$ and $0.95$ quantiles

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
  x1 = 100
  mu = samples$a*x1 + samples$b
  plot(density(mu))
  mean(mu)
  quantile(mu,c(0.05, 0.95))
```

e)  As in d) calculate $\mu_|x = a+bx$ at 40 positions from $x=-10$ to $x=10$. Extract the mean and the 0.05 and 0.95 quantiles. Plot the mean and the quantiles against $x$. Also add the data together with the maximum likelihood solution to the plot.

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
  xs = seq(-10,10,length.out = 40)
  draw_lines = function(xs, samples){
   df = NULL
    for (x1 in xs){
      mu=(samples$b + samples$a*x1)
      df1 = data.frame(mean = mean(mu))
      q = quantile(mu,c(0.05, 0.95))
      df1$q5 = q[1]
      df1$q95 = q[2]
      df = rbind(df,df1)
      df  
    }
    lines(xs, df$mean, col='green', lty=2)
    lines(xs, df$q5, col='green', lty=2)
    lines(xs, df$q95, col='green', lty=2)
  }
  curve(a*x+b, from = -10, to=10, main=paste0('Number of Datapoints ', N)) #ML
  points(x,y) #Data
  draw_lines(xs, samples)
```

f)  Repeat the Bayesian analysis but now only choose 5 data points. What happens with the uncertainty.

```{r, echo=lsg, eval=lsg,results="hide", warning=FALSE}
  set.seed(6)
  N = 5 ## <-- The only change, rest is copy and paste
  x = seq(-2,2, length.out = N)
  y = rnorm(N, mean = 2 * x + 1, sd = 1)
  d = data.frame(x=x,y=y) 
  
  #post = sampling(model, data=list(N=N,x=x, y=y))
  #samples = extract(post)
  #samples = as.data.frame(post)
  
  post = m_rcmdstan$sample(data=list(N=N,x=x, y=y))
  draws = post$draws(format = 'df')
  curve(a*x+b, from = -10, to=10, main=paste0('Number of Datapoints ', N)) #ML
  points(x,y) #Data
  draw_lines(xs, draws)
```

<!-- g) Posterior Predictive Distribution: The solutions in e-f only describe how the distribution of the mean value $\mu|x$ changes, when you change $x$, they do not describe how the posterior predictive distribution $p(y|D)$ looks like. To simulate from that distribution sample $y_i$ from many posterior samples from $a_i$, $b_i$ which create $\mu_i|x$  (as above) and then sample from $y ~ Normal(\mu_i, \sigma_i)$. We restrict ourself to the point $x=1$ As in plot d) the density of $\mu$ and $y$ at $x=1$. -->

```{r, echo=FALSE, eval=FALSE}
  x1 = 1
  mu = samples$b + samples$a*(x1)
  y1 = rnorm(mean=mu, samples$sigma)
  plot(density(y1), col='red')
  lines(density(mu))
```

<!-- h) [Optional] Posterior Predictive for all x. This is not needed, in exercise of week 3 you will do this. -->

```{r, echo=FALSE, eval=FALSE}
  draw_lines_ppd = function(xs, samples){
   df = NULL
    for (x1 in xs){
      mu=(samples$b + samples$a*x1)
      #mu = samples$a + samples$b*(x1)
      y1 = rnorm(mean=mu, samples$sigma)
      df1 = data.frame(mean = mean(y1))
      q = quantile(y1,c(0.05, 0.95))
      df1$q5 = q[1]
      df1$q95 = q[2]
      df = rbind(df,df1)
      df  
    }
    lines(xs, df$mean, col='red', lty=2)
    lines(xs, df$q5, col='red', lty=2)
    lines(xs, df$q95, col='red', lty=2)
  }
  #samples = as.data.frame(post)
  curve(a*x+b, from = -10, to=10, main=paste0('Number of Datapoints ', N)) #ML
  points(x,y) #Data
  draw_lines(xs,  draws)
  draw_lines_ppd(xs, draws)
```
