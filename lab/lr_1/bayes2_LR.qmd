---
output:
  pdf_document:
    highlight: pygments
  html_document: 
    default
params:
  lsg: TRUE
---

```{r, echo=FALSE, eval=TRUE, message=FALSE}
  #The variable lsg is used to control the visibility of the solutions and needs to be set
  if (exists("lsg") == FALSE){  
    lsg <- params$lsg
  }
```

## Linear Regression

In this exercise, we will compare the Maximum Likelihood (ML) solution with a Bayesian solution for linear regression. We will generate synthetic data, fit models using both methods, and analyze the results to understand the differences in their approaches and outcomes.  

a)  **Data Generation** Use the same data generating process as in `Maximum Likelihood Principle (linear regression)` but with only 5 datapoints at $x=-2,-1,0,1,2$. Simulate 5 data points $(x,y)$ under the assumptions from linear regression $y \sim Norm(\mu=2.0 \cdot x + 1.0, \sigma=1)$. Let $x$ be in the range from -2 to 2. Hint: use the function $rnorm$.

```{r, echo=lsg, eval=lsg}
  set.seed(1)
  N = 5
  x = seq(-2,2, length.out = N)
  y = rnorm(N, mean = 2 * x + 1, sd = 1)
```

b)  Fit the maximum likelihood solution with the R command `lm`. Plot the Data and $\mu_{ML} = E(y|x)$

```{r, echo=lsg, eval=lsg, warning=FALSE}
  maxl = lm(y ~ x)
  d = coef(maxl)
  a = d[2]
  b = d[1]
  curve(a*x+b, from = -5, to=5)
  points(x,y)
  a
  b
```

c)  Do a Bayesian analysis with a extremely flat prio, e.g. $a,b \sim unif(-100,100)$ and $\sigma \sim uniform(0,100)$. Extract samples of $a,b$ from the posterior and plot them. Compare the mean of the Bayesian Analysis for $a$ and $b$ with the Maximum Likelihood Solution.

<!-- #### Hints: -->
<!-- Stan: In case you are stuck, the stan-file can be found at <https://github.com/oduerr/da/blob/master/lab/lr_1/lr.stan> -->

Creating samples from posterior:
```{r, echo=TRUE, eval=FALSE}
  library(cmdstanr)
  m_rcmdstan <- cmdstan_model('Link_to_stan_file.stan')
  post = m_rcmdstan$sample(data=list(N=N,x=x, y=y)) #Sampling from the posterior
  samples = post$draws(format = "df") #Extracting the samples in standard data.frame format
```

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
library(cmdstanr)
m_rcmdstan <- cmdstan_model('~/Documents/GitHub/da/lab/lr_1/lr.stan')

#model = stan_model(model_code = model_code)
#post = stan(model_code = model_code, data=list(N=N,x=x, y=y))
post = m_rcmdstan$sample(data=list(N=N,x=x, y=y))
#samples = extract(post)
samples = post$draws(format = "df")

#In nice
library(ggplot2)
ggplot(samples,aes(x=a,y=b)) + 
  geom_point(size=0.1, alpha=0.2) + 
  geom_density2d() 

#Also ok (non-ggplot)
# plot(samples$a,samples$b, pch='.')
a
mean(samples$a) 
hist(samples$a)
plot(density(samples$a))

b
mean(samples$b)
lm(y ~ x)
```

d)  **x-dependance** of $\mu$: At $x=1$ calculate $\mu_|x = a+bx$ from the posterior samples and plot it. Extract the mean and the $0.05$ and $0.95$ quantiles

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
  x1 = 100
  mu = samples$a*x1 + samples$b
  plot(density(mu))
  mean(mu)
  quantile(mu,c(0.05, 0.95))
```

e)  As in d) calculate $\mu_|x = a+bx$ at 40 positions from $x=-10$ to $x=10$. Extract the mean and the 0.05 and 0.95 quantiles. Plot the mean and the quantiles against $x$. Also add the data together with the maximum likelihood solution to the plot.

```{r, echo=lsg, eval=lsg, results="hide", warning=FALSE}
  xs = seq(-5,5,length.out = 40)
  draw_lines = function(xs, samples){
   df = NULL
    for (x1 in xs){
      mu=(samples$b + samples$a*x1)
      df1 = data.frame(mean = mean(mu))
      q = quantile(mu,c(0.05, 0.95))
      df1$q5 = q[1]
      df1$q95 = q[2]
      df = rbind(df,df1)
      df  
    }
    lines(xs, df$mean, col='green', lty=2)
    lines(xs, df$q5, col='green', lty=2)
    lines(xs, df$q95, col='green', lty=2)
  }
  curve(a*x+b, from = -5, to=5, main=paste0('Number of Datapoints ', N)) #ML
  points(x,y) #Data
  draw_lines(xs, samples)
```



f) **Posterior Predictive Distribution**: The solutions in d-e only describe how the distribution of the mean value $\mu|x$ changes, when you change $x$, they do not describe how the posterior predictive distribution $p(y|D)$ looks like. To simulate from that distribution sample $y_i$ from many posterior samples from $a_i$, $b_i$ which create $\mu_i|x$  (as above) and then sample from $y ~ Normal(\mu_i, \sigma_i)$. We restrict ourself to the point $x=1$ As in plot d) the density of $\mu$ and $y$ at $x=1$.

```{r, echo=FALSE, eval=FALSE}
  x1 = 1
  mu = samples$b + samples$a*(x1)
  y1 = rnorm(mean=mu, samples$sigma)
  plot(density(y1), col='red')
  lines(density(mu))
```

g) [Optional] Posterior Predictive for all x. 

```{r, echo=lsg, eval=lsg}
  draw_lines_ppd = function(xs, samples){
   df = NULL
    for (x1 in xs){
      mu=(samples$b + samples$a*x1)
      #mu = samples$a + samples$b*(x1)
      y1 = rnorm(mean=mu, samples$sigma)
      df1 = data.frame(mean = mean(y1))
      q = quantile(y1,c(0.05, 0.95))
      df1$q5 = q[1]
      df1$q95 = q[2]
      df = rbind(df,df1)
      df  
    }
    lines(xs, df$mean, col='red', lty=2)
    lines(xs, df$q5, col='red', lty=2)
    lines(xs, df$q95, col='red', lty=2)
  }
  #samples = as.data.frame(post)
  curve(a*x+b, from = -5, to=5, main=paste0('Number of Datapoints ', N)) #ML
  points(x,y) #Data
  draw_lines(xs,  samples)
  draw_lines_ppd(xs, samples)
```
