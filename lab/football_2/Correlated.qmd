---
output:
  pdf_document:
    highlight: pygments
    toc: true 
  html_document: 
    default:
      toc: true      
params:
  lsg: TRUE
---

```{r, echo=FALSE, eval=TRUE, message=FALSE}
  #The variable lsg is used to control the visibility of the solutions and needs to be set
  if (exists("lsg") == FALSE){ 
    lsg <- params$lsg
  }
```

# Correlated Hierarchical Model

This notebook introduces correlated hierarchical model for football for the Bundesliga 2023/2024. 

::: {.callout-caution collapse="false"}
# Direct Parametrization
The direct parametrization used here is not the best choice of the Gaussian but it is easier to understand so we use it here for didactic reasons. A numerical more stable choice is the Cholesky decomposition. See e.g. <https://oduerr.github.io/da/Euro24/euro24.html> and refences therein. 
:::

## Data preparation

### Loading
We first load the data. The data comes from <https://www.football-data.co.uk/germanym.php>. You can you the data via:
```{r,  echo = TRUE, eval=FALSE}
  data_complete = read.csv('https://raw.githubusercontent.com/oduerr/da/master/lab/football_2/bundesliga2023.csv', stringsAsFactors = FALSE)
```

```{r, echo = FALSE, eval=TRUE, warning=FALSE}
  #data_complete = read.csv('https://raw.githubusercontent.com/oduerr/da/master/lab/football_2/bundesliga2023.csv', stringsAsFactors = FALSE)
  data_complete = read.csv('~/Documents/GitHub/da/website/Euro24/bundesliga2023.csv', stringsAsFactors = FALSE)
```

### Preprocessing
You need to apply the following preprocessing
```{r, echo=TRUE, eval=TRUE, message=FALSE}
  library(magrittr)
  library(dplyr)
  data <- data_complete %>% dplyr::select(Home = HomeTeam, score1 = FTHG, score2 = FTAG, Away = AwayTeam)
  teams = unique(data$Home)
  ng = nrow(data)
  # We need to encode the teams as numbers
  ht = unlist(sapply(1:ng, function(g) which(teams == data$Home[g])))
  at = unlist(sapply(1:ng, function(g) which(teams == data$Away[g])))
  cat('We have G =', ng, 'games \n')
  nt = length(unique(data$Home))
  cat('We have T = ', nt, 'teams \n')
```

### Convenience functions

```{r, echo=TRUE, eval=TRUE}
#### getting data for STAN ########
# we will save the last np games to predict
get_data = function(np){
  ngob = ng-np #number of games to fit
  my_data = list(
    nt = nt, 
    ng = ngob,
    ht = ht[1:ngob], 
    at = at[1:ngob], 
    s1 = data$score1[1:ngob],
    s2 = data$score2[1:ngob],
    np = np,
    htnew = ht[(ngob+1):ng],
    atnew = at[(ngob+1):ng],
    s1new = data$score1[(ngob+1):ng],
    s2new = data$score2[(ngob+1):ng],
    
    score1 = data$score1, #Have all in one place
    score2 = data$score2
  )
  return (my_data)
}
```


# A Model for the goals scored 

We will assume that the number of goals scored by the home team $s_1$ and the away team $s_2$ follows a Poisson distribution. This has been shown to be a good model for the number of goals scored in a football match. We model the rate parameter $\theta$ of the Poisson distribution, related to the attack and defense strengths of the teams, as follows:

$$
	s_1 \sim \text{Pois}(\theta_1) \quad\text{goals scored by the home team}
$$
	
$$
	s_2 \sim \text{Pois}(\theta_2) \quad\text{goals scored by the away team}
$$ 

This is equivalent to performing two separate Poisson regressions, one for each team.

We assume that:

$$
	\theta_1 = \exp(\text{home} + \text{att}_\text{ht} - \text{def}_\text{at})
$$
	
$$
	\theta_2 = \exp(\text{att}_\text{at} - \text{def}_\text{ht})
$$



# Correlated Hierarchical Model

Football (soccer) is an ideal example to get a hand on hierarchical models with correlated priors. 


## Multivariate Priors

Let's start to understand multivariate priors, by telling the data generating story. We start to describe how the prior is generated. Without seeing any games, the attack and defence ability of a team $j$ is sampled from a multivariate normal MVN with the covariance matrix $K$ .

$$
 \begin{bmatrix}att_j \\ def_j \end{bmatrix} \sim N(0,K) = N (\begin{bmatrix}0 \\ 0 \end{bmatrix} ,\begin{bmatrix} \sigma^2_{\tt{att}} & cov(att,def) \\ cov(att,def) & \sigma^2_{\tt{def}} \end{bmatrix} 
$$

Averaged over all teams in the league, the attack abilities sum up to zero (at least for a huge league). The same is true for the defense abilities. To describe this model have 3 parameters: the spread of the defense and attack abilities $\sigma_\tt{att} \ge 0$ and $\sigma_\tt{def} \ge 0$ and the covariance $cov(\tt{att}, \tt{def})\in [-\infty, \infty]$ between both quantities. A more interpretable quantity then the covariance is the correlation $\rho_{1,2}=cor(\tt{att},\tt{def})\in[-1,1]$, which can be calculated via $cov(\tt{att}, \tt{def}) = cor(\tt{att},\tt{def}) \sigma_\tt{att} \sigma_\tt{def}$. In principle, we can construct the covariance matrix from the correlation matrix $\rho$ as follows ($\rho$ is the matrix in the middle) :

$$ K = \begin{bmatrix} \sigma_{\tt{att}} & 0 \\ 0 & \sigma_{\tt{def}} \end{bmatrix} \cdot \begin{bmatrix} 1 & \rho_{1,2} \\ \rho_{1,2} & 1 \end{bmatrix} \cdot \begin{bmatrix} \sigma_{\tt{att}} & 0 \\ 0 & \sigma_{\tt{def}} \end{bmatrix}
$$

## Understand the stan code

Have a look at the stan code 'hier_model_cor_nocholsky.stan' <https://github.com/oduerr/da/tree/master/lab/football_2> and try to understand it and sample via:

```{r, warning=FALSE, message=FALSE, results="hide"}
my_data = get_data(1) 
options(mc.cores = parallel::detectCores())
library(cmdstanr)
kn_s.model <- cmdstan_model('~/Documents/GitHub/da/lab/football_2/hier_model_cor_nocholsky.stan')
cfit = kn_s.model$sample(data=my_data) 
```

#### Checking the fit 
Check the results of the sampling. What do you observe?

```{r, checks}
#shinystan::launch_shinystan(cfit) 
cfit
diagnostics <- cfit$diagnostic_summary()
bayesplot::mcmc_rhat_hist(bayesplot::rhat(cfit))
bayesplot::mcmc_neff_hist(bayesplot::neff_ratio(cfit))
```


```{r,echo=lsg, eval=FALSE}
# There are quite some divergent transitions. 
```

# Analysis of the posterior (Tasks)

## Task 1: Evaluate the Home Advantage
```{r,echo=lsg, eval=lsg} 
# The home advantage is given by the parameter home.
# Extract the posterior of the home advantage and plot it.
home = cfit$draws('home')
plot(density(home))
```

## Task 2: Plot the attack and defense abilities
Plot the attack and defense abilities of the team in the Bundesliga season 2024.  You can extract the average values and the quantiles of the posterior draws using e.g.the following code:

```{r, echo=TRUE, eval=TRUE, message=FALSE}
library(tidyverse)
library(tidybayes)

# Step 1: Gather draws and calculate summary statistics with credible intervals
As = cfit %>% 
  tidybayes::gather_draws(A[i, j]) %>%
  group_by(i, j) %>%
  summarise(
    average_value = mean(.value),
    lower = quantile(.value, 0.25),  # Lower bound 
    upper = quantile(.value, 0.75),  # Upper bound 
    .groups = "drop"
  )
A_avg = xtabs(average_value ~ i + j, data = As)
A_lower = xtabs(lower ~ i + j, data = As)
A_upper = xtabs(upper ~ i + j, data = As)
A_avg
```

Hint:
To plot the team names from the indices you can use the following code
```{r}
teams[1:length(teams)]
```

```{r, echo=lsg, eval=lsg, fig.width=7, fig.height=7}
plot(A_avg[1,], A_avg[2,], pch=20, xlab='Attack', ylab='Defence', main='Attack vs Defence', xlim=c(-0.5,1), ylim=c(-0.5,0.5))


# Step 4: Add team labels
text(A_avg[1,], A_avg[2,], labels=teams, cex=0.7, adj=c(-0.05, -0.8))
# Plot vertical error bars
arrows(A_avg[1,], A_lower[2,], A_avg[1,], A_upper[2,], angle=90, code=3, length=0.05, col="lightblue")

# Plot horizontal error bars
arrows(A_lower[1,], A_avg[2,], A_upper[1,], A_avg[2,], angle=90, code=3, length=0.05, col="lightblue")
```

## Task 3: Predict the probability of home win, draw, and away win for a game
 
Predict the number of goals for the next games, between the teams Freiburg and Leverkusen with Freiburg being the home team. You can use the following code to extract the posterior distributions for the attack and defense abilities of the teams Leverkusen and Freiburg.

```{r, echo=TRUE, eval=TRUE}
library(tidybayes)
library(dplyr)

# Extract team indices
id1 = which(teams == 'Leverkusen')
id2 = which(teams == 'Freiburg')

attack_leverkusen <- cfit %>% spread_draws(A[i, j]) %>% filter(j == id1, i == 1) %>% dplyr::select(A) %>% pull()
defense_leverkusen <- cfit %>% spread_draws(A[i, j]) %>% filter(j == id1, i == 2) %>% dplyr::select(A) %>% pull()
attack_freiburg <- cfit %>% spread_draws(A[i, j]) %>% filter(j == id2, i == 1) %>% dplyr::select(A) %>% pull()
defense_freiburg <- cfit %>% spread_draws(A[i, j]) %>% filter(j == id2, i == 2) %>% dplyr::select(A) %>% pull()
home = cfit %>% spread_draws(home) %>% dplyr::select(home) %>% pull()

### Example code

num_draws = num_home = num_away = 0
for (i in 1:length(home)){
  # goals_freiburg =
  # goals_leverkusen =
  # if (...)
  #   num_home = num_home + 1
  # if (...)
  #   num_away = num_away + 1
  # if (...)
  #   num_draws = num_draws + 1
}
```

To caclulate the posterior predictive distribution for the number of goals scored by Leverkusen and Freiburg, you can draw the goals for a game from the Poisson distribution with the rate parameter $\theta$ given by the attack and defense abilities of the teams. 

```{r, echo=lsg, eval=lsg}
num_draws = num_home = num_away = 0
for (i in 1:length(home)){
    goals_freiburg = rpois(1, lambda = exp(home + attack_freiburg - defense_leverkusen))
    goals_leverkusen = rpois(1, lambda = exp(attack_leverkusen - defense_freiburg))
    if (goals_freiburg > goals_leverkusen)
      num_home = num_home + 1
    if (goals_freiburg < goals_leverkusen)
      num_away = num_away + 1
    if (goals_freiburg == goals_leverkusen)
      num_draws = num_draws + 1
}
p_bayes = c(num_home, num_draws, num_away)/length(home)
p_bayes
```

## Task 4: Predict the results of the next games w/o Bayes
In this task, we will predict the results of the next games between Freiburg and Leverkusen without using Bayesian methods. Instead, we will use the mean values of the posterior distributions for the attack and defense abilities of the teams as well as the home advantage parameter. Discuss the results.


```{r, echo=lsg, eval=lsg}
num_draws = num_home = num_away = 0
attack_freiburg_mean <- mean(attack_freiburg)
defense_freiburg_mean <- mean(defense_freiburg)
attack_leverkusen_mean <- mean(attack_leverkusen)
defense_leverkusen_mean <- mean(defense_leverkusen)
home_mean <- mean(home)
set.seed(123) # For reproducibility
num_simulations <- 10000 # Number of simulations
num_home <- num_draws <- num_away <- 0

# Simulate the outcomes
for (i in 1:num_simulations) {
  goals_freiburg <- rpois(1, lambda = exp(home_mean + attack_freiburg_mean - defense_leverkusen_mean))
  goals_leverkusen <- rpois(1, lambda = exp(attack_leverkusen_mean - defense_freiburg_mean))
  
  if (goals_freiburg > goals_leverkusen) {
    num_home <- num_home + 1
  } else if (goals_freiburg < goals_leverkusen) {
    num_away <- num_away + 1
  } else {
    num_draws <- num_draws + 1
  }
}


p = c(num_home, num_draws, num_away)/num_simulations
p

results <- c("Home Win", "Draw", "Away Win")
barplot_height <- rbind(p, p_bayes)
colnames(barplot_height) <- results
rownames(barplot_height) <- c("Mean Values", "Bayesian")

# Create the bar plot
barplot(barplot_height, beside=TRUE, col=c("skyblue", "red"), 
        legend.text=TRUE, args.legend=list(x="topright", bty="n"),
        main="Probability of Game Outcomes: Mean Values vs Full Bayesian",
        xlab="Result", ylab="Probability", ylim=c(0, 1))

print(paste("Mean Values: Home Win: ", p[1], ", Draw: ", p[2], ", Away Win: ", p[3]))
print(paste("Bayesian: Home Win: ", p_bayes[1], ", Draw: ", p_bayes[2], ", Away Win: ", p_bayes[3]))
-sum(p*log(p)) #0.87
-sum(p_bayes*log(p_bayes)) #0.95
-log(1/3) #1.09 
```

```{echo=lsg, eval=FALSE, as.is=TRUE}
As you can see the Bayesian model is more uncertain since it includes the uncertainty of the parameters. This is also called epistemic uncertainty. 
```