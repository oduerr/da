---
title: "Comparison of different models Bundesliga Dataset"
author: "Oliver DÃ¼rr"
format: 
  html:
    toc: true
    toc-title: "Table of Contents"
    toc-depth: 3
    fig-width: 6
    fig-height: 3
    code-fold: true
    code-tools: true
    mathjax: true
  # pdf:
  #   toc: true
  #   toc-title: "Table of Contents"
  # filters:
  #- webr
---

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
  library(tidyverse)
  library(kableExtra)
  set.seed(42)
```

The experiments take some time to run, therefore we used the R-Script to producte the results <https://github.com/oduerr/da/blob/master/website/Euro24/eval_performance_runner.R>.

## Loading the data
```{r, asis=TRUE}
  df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23.csv')
  df <- mutate(df, name = sub("^.*/", "", name))
  
  #Add additional run with home advantage
  df2 = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_home_adv.csv')
  df2 <- mutate(df2, name = sub("^.*/", "", name))
  df <- rbind(df, df2)
  
  # Add additional run with 18 games ahead
  df3 = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_18Ahead.csv')
  df = df3
  print("Using 24_18Ahead that means all data is averaged 18 games ahead.")
  
  #df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_bets05_18Ahead.csv')
  
  df_raw = read.csv('~/Documents/GitHub/da/website/Euro24/bundesliga2023.csv')
  head(df_raw) %>% kable()
```

## Exploratory Analysis
```{r}
#str(df_raw)
#Full Time Home Goals, Full Time Away Goals
home_wins = df_raw$FTHG > df_raw$FTAG
away_wins = df_raw$FTHG < df_raw$FTAG
draws = df_raw$FTHG == df_raw$FTAG

# Betting on home wins would have some log(0) --> -Inf

# Betting according frequency
# This is a bit cheating, since we use the future data to calculate
ps_naive = c(sum(home_wins), sum(draws), sum(away_wins))/nrow(df_raw)
NLL_NAIVE = sum(
  -home_wins*log(ps_naive[1])
  -draws*log(ps_naive[2])
  -away_wins*log(ps_naive[3])
)/nrow(df_raw)
NLL_NAIVE
```


## Exploratory Analysis
```{r}
#str(df_raw)

# Load necessary libraries
library(dplyr)

# Load your data
# df_raw <- read.csv('path_to_your_data.csv') # Replace with the actual data loading method

# Example data frame structure for reference
# df_raw <- data.frame(
#   B365H = c(7.5, 2.7, 2.3), B365D = c(6.5, 3.6, 3.6), B365A = c(1.3, 2.45, 2.9),
#   FTR = c("A", "D", "A")
# )

# Calculate implied probabilities
df_raw <- df_raw %>%
  mutate(
    ImpliedProb_H = 1 / B365H,
    ImpliedProb_D = 1 / B365D,
    ImpliedProb_A = 1 / B365A
  ) %>%
  mutate(
    SumImpliedProbs = ImpliedProb_H + ImpliedProb_D + ImpliedProb_A,
    ImpliedProb_H = ImpliedProb_H / SumImpliedProbs,
    ImpliedProb_D = ImpliedProb_D / SumImpliedProbs,
    ImpliedProb_A = ImpliedProb_A / SumImpliedProbs
  )

# Convert actual outcomes to binary indicators
df_raw <- df_raw %>%
  mutate(
    Outcome_H = ifelse(FTR == "H", 1, 0),
    Outcome_D = ifelse(FTR == "D", 1, 0),
    Outcome_A = ifelse(FTR == "A", 1, 0)
  )

# Calculate negative log-likelihood for the given strategy
df_raw <- df_raw %>%
  mutate(
    NLL_H = Outcome_H * log(ImpliedProb_H),
    NLL_D = Outcome_D * log(ImpliedProb_D),
    NLL_A = Outcome_A * log(ImpliedProb_A),
    NLL = -(NLL_H + NLL_D + NLL_A)
  )

# Mean negative log-likelihood for the given strategy
mean_nll <- mean(df_raw$NLL, na.rm = TRUE)

cat("Mean Negative Log-Likelihood for given strategy:", mean_nll, "\n")

# Calculate negative log-likelihood for uniform betting (1/3 for each outcome)
baseline_prob <- 1 / 3
df_raw <- df_raw %>%
  mutate(
    BaselineNLL_H = Outcome_H * log(baseline_prob),
    BaselineNLL_D = Outcome_D * log(baseline_prob),
    BaselineNLL_A = Outcome_A * log(baseline_prob),
    BaselineNLL = -(BaselineNLL_H + BaselineNLL_D + BaselineNLL_A)
  )

# Mean negative log-likelihood for uniform betting
mean_baseline_nll <- mean(df_raw$BaselineNLL, na.rm = TRUE)

cat("Mean Negative Log-Likelihood for uniform betting:", mean_baseline_nll, "\n")

# Calculate negative log-likelihood for always betting on home win
df_raw <- df_raw %>%
  mutate(
    AlwaysHomeNLL_H = -log(baseline_prob) * Outcome_H,
    AlwaysHomeNLL_L = -log(1 - baseline_prob) * (1 - Outcome_H),
    AlwaysHomeNLL = AlwaysHomeNLL_H + AlwaysHomeNLL_L
  )
mean_always_home_nll <- mean(df_raw$AlwaysHomeNLL, na.rm = TRUE)

cat("Mean Negative Log-Likelihood for always betting on home win:", mean_always_home_nll, "\n")

# Calculate negative log-likelihood for always betting on away win
df_raw <- df_raw %>%
  mutate(
    AlwaysAwayNLL_A = -log(baseline_prob) * Outcome_A,
    AlwaysAwayNLL_L = -log(1 - baseline_prob) * (1 - Outcome_A),
    AlwaysAwayNLL = AlwaysAwayNLL_A + AlwaysAwayNLL_L
  )
mean_always_away_nll <- mean(df_raw$AlwaysAwayNLL, na.rm = TRUE)

cat("Mean Negative Log-Likelihood for always betting on away win:", mean_always_away_nll, "\n")

# Calculate negative log-likelihood for always betting on draw
df_raw <- df_raw %>%
  mutate(
    AlwaysDrawNLL_D = -log(baseline_prob) * Outcome_D,
    AlwaysDrawNLL_L = -log(1 - baseline_prob) * (1 - Outcome_D),
    AlwaysDrawNLL = AlwaysDrawNLL_D + AlwaysDrawNLL_L
  )
mean_always_draw_nll <- mean(df_raw$AlwaysDrawNLL, na.rm = TRUE)

cat("Mean Negative Log-Likelihood for always betting on draw:", mean_always_draw_nll, "\n")
```

## Model Comparisons
We use the negative log likelihood (NLL) as a measure of the predictive performance of the models. The lower the NLL, the better the model. However, strictly speaking it is the negative log posterior predictive density (divided by $n$) evaluated at the $n$ games after the training data.

$$
\text{NLL} = -\frac{1}{n}\sum_{i=1}^n \log p(y_i | x_i, \theta)
$$



```{r hier-vs-non, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
   # Assuming df is your dataframe
  df %>% filter(type == 'NLL_PRED') %>% 
    ggplot(aes(x = ntrain, y = res, color = name)) + 
    geom_line() + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset', 
      x = 'Number of training data', 
      y = 'Negative Log Likelihood'
    ) + 
    #ylim(2.9, 3.5) +
    #xlim(0,100) +
    theme(legend.position = "top") +
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area

```

### Observations
- Especially for small training data, the hierarchical model performs better than the non-hierarchical model. 
- The Correlated Dataset model performs slightly better than non-correlated one

- There is partically no difference in predictive performance when comparing the model with and without Cholesky decomposition. 

- The negative binomial model performs comparable to Poisson model.


## Comparison of predicted vs PSIS-LOO

```{r pred-vs-loo, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% filter(type %in% c('NLL_PRED', 'NLL_PSIS', 'NLL_PRED_STAN')) %>%
    ggplot(aes(x = ntrain, y = res, color = type)) + 
    geom_line(aes(linetype = type)) + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title =  'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Negative Log Likelihood'
    ) + 
    ylim(2.5, 4) +
    facet_wrap(~name) +
    theme(legend.position = "bottom") + 
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area
```

 
### Observations


## NNL for win, draws and losses 


```{r resultbases, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% 
    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %>%
    ggplot(aes(x = ntrain, y = res, color = type)) + 
    geom_line(aes(linetype = name)) + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Negative Log Likelihood'
    ) + 
    ylim(0.75, 1.5) +
    geom_hline(yintercept = NLL_NAIVE, col = "green", alpha=0.5) +
    theme(legend.position = "bottom") + 
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area


#### Booki mean
  dfp = df %>% 
    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %>%
    group_by(name, type) %>% 
    summarise(nll = mean(res)) 
  
  dfp = rbind(dfp, data.frame(name = "NLL_NAIVE", type = "NLL_NAIVE", nll = NLL_NAIVE))
  # Remove all but one NLL_BOOKIE
  kableExtra::kable(dfp, digits = 3)
``` 

### Observations
- The NLL for the bookie is always better than the NLL of the models, so we should not bet.

### Ranked probability score

```{r result rps, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% 
    filter(type %in% c('RPS', 'rps_booki')) %>%
    ggplot(aes(x = ntrain, y = res, color = type)) + 
    geom_line(aes(linetype = name)) + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Ranked Probability Score'
    ) + 
    #ylim(0.75, 1.5) +
    theme(legend.position = "bottom") + 
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area
``` 

Averaged for the complete season:
```{r rps_season}
  library(dplyr)
  dfres = df %>% 
    filter(type %in% c('RPS', 'rps_booki')) %>%
    group_by(name, type) %>% 
    summarise(rps = mean(res)) 
  
  rps_booki = dfres %>% filter(type == 'rps_booki') 
  
  dfres2 = dfres %>% filter(type == 'RPS') %>% dplyr::select(name, rps) 
  dfres2 = rbind(dfres2, rps_booki[1,])

  # Sort by RPS
  dfres2 = dfres2[order(dfres2$rps),]
  kable(dfres2)
```



## Betting Returns
 

```{r betting, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% 
    filter(type %in% c('BET_RETURN')) %>%
    ggplot(aes(x = ntrain, y = res, color = name)) + 
    geom_line(aes(linetype = name)) +  
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Betting Returns'
    ) + 
    #ylim(0.75, 1.5) +
    theme(legend.position = "bottom") +  
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area
  df %>% 
    filter(type %in% c('BET_RETURN')) %>%
    group_by(name) %>% 
    summarise(mean(res))
```

### Observations
We see quite some fluctuation in the betting return. Since the NLL shows that the odds from the booki are always better then the NLLs of the models we should not bet.  


## Technical Details
 

```{r min_sum, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% 
    filter(type %in% c('MIN_SUM_PROB')) %>%
    ggplot(aes(x = ntrain, y = res, color = name)) + 
    geom_line(aes(linetype = name)) + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Sum of Probabilities from 0 to 10 goals (should be 1)'
    ) + 
    ylim(0.75, 1.01) +
    theme(legend.position = "bottom") + 
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area
```

```{r num_divergent, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
  df %>% 
    filter(type %in% c('num_divergent')) %>%
    ggplot(aes(x = ntrain, y = res, color = name)) + 
    geom_line(aes(linetype = name)) + 
    geom_point() + 
    theme_minimal() + 
    labs(
      title = 'Comparison of different models for the Bundesliga 2023 dataset',
      x = 'Number of training data', 
      y = 'Number of Divergent Transitions (sqrt scale)'
    ) + 
    theme(legend.position = "bottom") + 
    scale_y_sqrt() + 
    coord_cartesian(clip = "off") # Allow lines to go outside the plot area 
```

```{r , fig.width=8, fig.height=6, warning=FALSE, message=FALSE}

df %>% 
  filter(type %in% c('ebfmi')) %>%
  ggplot(aes(x = ntrain, y = res, color = name)) + 
  geom_line(aes(linetype = name)) + 
  geom_point() + 
  theme_minimal() + 
  labs(
    title = 'Comparison of different models for the Bundesliga 2023 dataset',
    x = 'Number of training data', 
    y = 'ebfmi'
  ) + 
  theme(legend.position = "bottom") + 
  geom_hline(yintercept = 0.3, linetype = "dashed", color = "red") +
  annotate("text", x = Inf, y = 0.33, label = "Acceptable", hjust = 1.1, color = "red") +
  annotate("text", x = Inf, y = 0.27, label = "Non-Acceptable", hjust = 1.1, color = "red") +
  coord_cartesian(clip = "off") # Allow lines to go outside the plot area
```