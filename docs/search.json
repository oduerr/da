[
  {
    "objectID": "Stan_Primer_Full.html",
    "href": "Stan_Primer_Full.html",
    "title": "Stan primer",
    "section": "",
    "text": "MCMC with Stan / Simple diagnostics"
  },
  {
    "objectID": "Stan_Primer_Full.html#cmdstan",
    "href": "Stan_Primer_Full.html#cmdstan",
    "title": "Stan primer",
    "section": "Cmdstan",
    "text": "Cmdstan\n\nlibrary(cmdstanr)\noptions(mc.cores = parallel::detectCores()) #Make it faster\nm_rcmdstan &lt;- cmdstan_model(file_stan) #Compiling\ns_rcmdstan = m_rcmdstan$sample(data = data) #Sampling\ndf = s_rcmdstan$draws(format = \"df\") #Extracting Samples as data.frame\n#shinystan::launch_shinystan(s_rcmdstan) #Shiny Stan"
  },
  {
    "objectID": "Stan_Primer_Full.html#tidy-bayes",
    "href": "Stan_Primer_Full.html#tidy-bayes",
    "title": "Stan primer",
    "section": "Tidy Bayes",
    "text": "Tidy Bayes\nWorks with rstan and cmdstanr\n\nlibrary(tidybayes)\nspread_draws(s, c(a,b)) #Extracts a and b from s (stan or cmdstan samples)\nspread_draws(s, f[i])  #Extracts vector f and calls components i"
  },
  {
    "objectID": "Stan_Primer_Full.html#bayes-plot",
    "href": "Stan_Primer_Full.html#bayes-plot",
    "title": "Stan primer",
    "section": "Bayes plot",
    "text": "Bayes plot\nWorks with rstan and cmdstanr\n\n  s = s_rcmdstan$draws() #We need to call draws() 2023\n  bayesplot::mcmc_trace(s)\n  bayesplot::ess_bulk(s)"
  },
  {
    "objectID": "Stan_Primer_Full.html#rendering-quarto-for-exercises",
    "href": "Stan_Primer_Full.html#rendering-quarto-for-exercises",
    "title": "Stan primer",
    "section": "Rendering Quarto for exercises",
    "text": "Rendering Quarto for exercises\nPrinting the model in markdown files (only important for rendering exercise sheets), see e.g.¬†around here\n{r, echo=lsg, eval=FALSE, code=readLines(‚Äú../world.stan‚Äù), collapse=TRUE}\nNot showing the many details in MCMC simulation with Stan. | ```{r, echo=lsg, eval=lsg, message=FALSE, warning=FALSE, results=‚Äúhide‚Äù}"
  },
  {
    "objectID": "Stan_Primer_Full.html#stan",
    "href": "Stan_Primer_Full.html#stan",
    "title": "Stan primer",
    "section": "Stan",
    "text": "Stan\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(javascript=FALSE) #Prevents freezes of RStudio (Jan 2023) \n\nm_rstan = stan_model(model_code = stan_code) #Compiling from string\nm_rstan = stan_model(file='mymodel.stan') #Compiling from file\ns_rstan = sampling(m_rstan, data=data) #Sampling from file\n\n#No MCMC just evaluating parameters\nsampling(model_1, data=dat, algorithm=\"Fixed_param\", chain=1, iter=1)"
  },
  {
    "objectID": "Stan_Primer_Full.html#other-cheat-sheets",
    "href": "Stan_Primer_Full.html#other-cheat-sheets",
    "title": "Stan primer",
    "section": "Other Cheat Sheets",
    "text": "Other Cheat Sheets\n\nhttp://www.sumsar.net/files/posts/2017-bayesian-tutorial-exercises/stan_cheat_sheet2.12.pdf\nhttps://github.com/sieste/Stan_cheatsheet"
  },
  {
    "objectID": "Stan_Primer_Full.html#data-used",
    "href": "Stan_Primer_Full.html#data-used",
    "title": "Stan primer",
    "section": "Data used",
    "text": "Data used\nSome Data for linear regression\n\nN = 4\nx = c(-2.,-0.66666, 0.666, 2.)\ny = c(-6.25027354, -2.50213382, -6.07525495,  7.92081243)\ndata = list(N=N, x=x, y=y)"
  },
  {
    "objectID": "Stan_Primer_Full.html#getting-samples-from-the-posterior",
    "href": "Stan_Primer_Full.html#getting-samples-from-the-posterior",
    "title": "Stan primer",
    "section": "Getting samples from the posterior",
    "text": "Getting samples from the posterior\nThere are currently (2023) two interfaces to stan from R. RStan (https://mc-stan.org/users/interfaces/rstan) which is a bit slower and does not use the latest stan compiler and rcmdstan (https://mc-stan.org/cmdstanr/articles/cmdstanr.html).\n\nThe technical steps\nThere are 3 steps:\n\nDefining the model\nCompiling the model. In this step C code is generated.\nRunning the simulation / sampling from the posterior\nExtracting the samples from the posterior\n\n\n\nDefinition\nTo define a model, you can add a string or create a .stan file. Another option is to use a Stan markdown chunk and output.var=my_model to the name of the model. Code completion and highlighting is working for files and code chunks.\n\nstan_code = \"data{\n  int&lt;lower=0&gt; N; \n  vector[N] y;\n  vector[N] x;\n}\n\nparameters{\n  real a; //Instead of using e.g. half Gaussian\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel{\n  //y ~ normal(mu, sigma);\n  y ~ normal(a * x + b, sigma);\n  a ~ normal(3, 10); \n  b ~ normal(0, 10); \n  sigma ~ normal(0,10);\n}\"\n\n\n\nCompiling\nCompiling works with:\n\nCompiling rstan stan_model(model_code = stan_code) or stan_model(file='mymodel.stan')\nCompiling cmdstan cmdstan_model(file_stan)\n\n\nCompiling with rstan\n\n  library(rstan)\n  m_rstan = stan_model(model_code = stan_code)\n\n\n\nCompiling with rcmdstan\nThere is no possibility to use a string for cmd_stan.\n\n  m_rcmdstan &lt;- cmdstan_model(stan_file='stan/simple_lr.stan')\n  m_rcmdstan$print() #Displays the used model\n\n\n\n\nSampling / running the chains\nFor rstan: sampling(m_rstan, data=data) For cmdstan: mod$sample(data = data_file, seed=123)\n\n  s_rcmdstan = m_rcmdstan$sample(data = data)\n\n\n  s_rstan = sampling(m_rstan)  \n\n\n\nDiagnostics of the chains\nThe package bayesplot can handle both interfaces\n\nTrace\n\n#traceplot(s_rstan, 'a')\n#bayesplot::mcmc_trace(s_rstan)\nbayesplot::mcmc_trace(s_rcmdstan$draws()) #similar result\ns_rstan\n# Rhat close to one and n_eff lager than half the number of draws; look fine\n\nKey Numbers\n\nRhat is something like the ratio of variation between the chains to withing the chains\nn_eff number of effective samples taking the autocorrelation into account (in cmd_stan the output is ess_bulk and ess_tail)\n\n\n\nShiny Stan\n\n  #Shiny Stan, quite overwhelming\n  library(shinystan)\n  launch_shinystan(s_rcmdstan)\n  \n  #If not working\n  #See https://discourse.mc-stan.org/t/will-launch-shinystan-work-soon-for-cmdstanr/17269\n  stanfit = rstan::read_stan_csv(s_rcmdstan$output_files())\n  launch_shinystan(stanfit)\n\n\n\n\nPosteriors (of the parameters)\nGetting the samples can be done as\n\nRstan extract(s_rstan)\ncmdstan s_rcmdstan$draws()\n\nAnother handy package is tidybayes which can handle the output of rstan and rcmdstan\n\nTidybayes\n\nlibrary(tidybayes)\nhead(spread_draws(s_rcmdstan, c(a,b))) #Non-tidy a and b in one row\n\n# A tibble: 6 √ó 5\n  .chain .iteration .draw     a      b\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1      1          1     1 1.99  -1.76 \n2      1          2     2 3.04   2.89 \n3      1          3     3 4.95  -3.39 \n4      1          4     4 0.934 -2.30 \n5      1          5     5 2.36   0.303\n6      1          6     6 5.31  -2.45 \n\nhead(gather_draws(s_rcmdstan, c(a,b))) #The ggplot like syntax\n\n# A tibble: 6 √ó 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable .value\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1      1          1     1 a          1.99 \n2      1          2     2 a          3.04 \n3      1          3     3 a          4.95 \n4      1          4     4 a          0.934\n5      1          5     5 a          2.36 \n6      1          6     6 a          5.31 \n\n#spread_draws(model_weight_sex, a[sex]) for multilevel models\n\n\n\nCommand Stan Functions\nFor more see: https://mc-stan.org/cmdstanr/articles/cmdstanr.html\n\ndf &lt;- s_rcmdstan$draws(format = \"df\")\ndf %&gt;% select(-c(.chain, .iteration, .draw)) %&gt;% cor()\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n            lp__           a           b      sigma\nlp__   1.0000000  0.12991773 -0.10625738 -0.6907346\na      0.1299177  1.00000000 -0.03857131 -0.1040964\nb     -0.1062574 -0.03857131  1.00000000  0.1078610\nsigma -0.6907346 -0.10409641  0.10786104  1.0000000\n\n\n\n\nStan functions\n\n  #plot(samples)\n  #samples = extract(s_rstan)\n  stan_dens(s_rstan)\n\n\n\n\n\n\n\n  #Note that these are marginals!\n\n\n\n‚ÄúManually‚Äù visualize the posterior\nWe extract samples from the posterior via extract (in some installation the wrong extract function is taken in that case use rstan::extract to use the right one). Visualize the posterior distribution of \\(a\\) from the samples.\n\n# Extract samples\npost = rstan::extract(s_rstan)\nT = length(post$a)\nhist(post$a,100, freq=F)\nlines(density(post$a),col='red')  \n\n\n\n\n\n\n\n\n\n\nPairs plot\nUsing the pairs plot, correlations in the variables can be found.\n\nnp_cp = bayesplot::nuts_params(s_rstan)\nbayesplot::mcmc_pairs(s_rstan, np = np_cp,pars = c(\"a\",\"b\"))\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Plots\n\n\nTask: Use the samples to create the following posterior predictive plots\nSome background first: posterior predictive distribution: \\[\n  p(y|x, D) =  \\int p(y|x,\\theta) p(\\theta|D) \\; d\\theta\n\\] Instead of integration, we sample in two turns\n\n\\(\\theta_i \\sim p(\\theta|D)\\)\n\\(y_{ix} \\sim p(y|x,\\theta_i)\\) #We do this for many x in practice\n\n\nCreation of the posterior predictive samples by hand\nYou can either do this part, or use stan to create the posterior predictive samples \\(y_{ix}\\) from the samples \\(\\theta_i\\) by hand.\nTip: Create two matrices yix and muix from the posterior samples of \\(a,b,\\sigma\\) with dimension (rows = number of posterior samples and cols = number of x positions).\n\nxs = -10:15 # The x-range 17 values from -1 to 15\nM = length(xs) \nyix = matrix(nrow=T, ncol = M) #Matrix from samples (number of posterior draws vs number of xs)\nmuix = matrix(nrow=T, ncol = M) #Matrix from mu (number of posterior draws vs number of xs)\nfor (i in 1:T){ #Samples from the posterior\n  a = post$a[i] #Corresponds to samples from theta\n  b = post$b[i]\n  sigma = post$sigma[i]\n  for (j in 1:M){ #Different values of X\n    mu = a * xs[j] + b\n    muix[i,j] = a * xs[j] + b\n    yix[i,j] = rnorm(1, mu, sigma) # Single number drawn\n  }\n}\n\nif (FALSE){\n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='mu=a*x+b')\n  for (i in 1:100){\n    lines(xs, muix[i,],lwd=0.25,col='blue')\n  }\n  \n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='ys')\n  for (i in 1:100){\n    points(xs, yix[i,], pch='.',col='red')\n  }\n}\n\nAfter you created the matrices yix and muix you can use the following function to draw the lines for the quantiles.\n\nplot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='quantiles (y and mu)')\nquant_lines = function(x2, y_pred, col='blue'){\n  m = apply(y_pred, 2,quantile, probs=c(0.50))\n  lines(x2, m,col=col)\n  q05 = apply(y_pred, 2, quantile, probs=c(0.25))\n  q95 = apply(y_pred, 2, quantile, probs=c(0.75))\n  lines(x2, q05,col=col)\n  lines(x2, q95,col=col)  \n}\n\nquant_lines(xs,yix, col='red')\nquant_lines(xs,muix, col='blue')\n\n\n\n\n\n\n\n\n\n\nCreation of the posterior predictive samples with Stan\nIt‚Äôs also possible to draw posterior predictive samples. One can use the generated quantities code block for that.\ndata{\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n  //For the prediced distribution (new)\n  int&lt;lower=0&gt; N2;\n  vector[N2] x2;\n}\n\ngenerated quantities {\n  real Y_predict[N2]; \n  for (i in 1:N2){\n    Y_predict = normal_rng(a * x2 + b, sigma);\n  }\n}\n\n  x2 = -10:15\n  N2 = length(x2)\n  fit2 = rstan::stan(file = '~/Dropbox/__HTWG/DataAnalytics/_Current/lab/06_03_Bayes_3/Stan_Primer_model_pred.stan',\n                         data=list(N=N,x=x, y=y, N2=N2,x2=x2),iter=10000)\n\n\n  d2 = rstan::extract(fit2)\n  y_pred = d2$Y_predict\n  dim(y_pred)\n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='quantiles (y and mu)')\n  quant_lines(x2,y_pred, col='red')\n\n\n\n\nNotes on creating excercises\n\nInclusion of Stan code\nIn case of cmdrstan the Stan code should be in an extra file to make the workflow easier and reuse compiled models. In case of rstan the Stan code can be included in the Rmd file as:\n{r, echo=TRUE, eval=FALSE, code=readLines(‚Äú../world.stan‚Äù), collapse=TRUE, echo=TRUE}"
  },
  {
    "objectID": "Stan_Primer_Full.html#rstudio-bug-2023",
    "href": "Stan_Primer_Full.html#rstudio-bug-2023",
    "title": "Stan primer",
    "section": "RStudio bug 2023",
    "text": "RStudio bug 2023\nSometimes R gets slow when rendering with stan code. At least in my case\nrstan_options(javascript=FALSE)\nNot observed anymore (2024)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "da_website",
    "section": "",
    "text": "This is a collection of bits and pieces useful in connection with data analysis."
  },
  {
    "objectID": "Euro24/comp_premier_league.html",
    "href": "Euro24/comp_premier_league.html",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "",
    "text": "The experiments take some time to run, therefore we used the R-Script to producte the results https://github.com/oduerr/da/blob/master/website/Euro24/eval_performance_runner.R."
  },
  {
    "objectID": "Euro24/comp_premier_league.html#loading-the-data",
    "href": "Euro24/comp_premier_league.html#loading-the-data",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\n  df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_premier_league_2019.csv')\n  df %&gt;% tail() %&gt;% kable()\n\n\n\n\n\n\n\nntrain\nres\ntype\nname\n\n\n\n\n2620\n375\n2.9454859\nNLL_PRED\nda/stan/football/hier_model_nb\n\n\n2621\n375\n0.8050755\nNLL_RESULTS\nda/stan/football/hier_model_nb\n\n\n2622\n375\n0.6949439\nNLL_BOOKIE\nda/stan/football/hier_model_nb\n\n\n2623\n375\n-0.8080000\nBET_RETURN\nda/stan/football/hier_model_nb\n\n\n2624\n375\n2.9883916\nNLL_PRED_STAN\nda/stan/football/hier_model_nb\n\n\n2625\n375\n2.9566802\nNLL_PSIS\nda/stan/football/hier_model_nb\n\n\n\n\n\n\n\n\nCode\n  df &lt;- mutate(df, name = sub(\"^.*/\", \"\", name))\n  \n  df_raw = read.csv('~/Documents/GitHub/da/website/Euro24/premierleague2019.csv')\n  head(df_raw) %&gt;% kable()\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\nReferee\nHS\nAS\nHST\nAST\nHF\nAF\nHC\nAC\nHY\nAY\nHR\nAR\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nVCH\nVCD\nVCA\nMaxH\nMaxD\nMaxA\nAvgH\nAvgD\nAvgA\nB365.2.5\nB365.2.5.1\nP.2.5\nP.2.5.1\nMax.2.5\nMax.2.5.1\nAvg.2.5\nAvg.2.5.1\nAHh\nB365AHH\nB365AHA\nPAHH\nPAHA\nMaxAHH\nMaxAHA\nAvgAHH\nAvgAHA\nB365CH\nB365CD\nB365CA\nBWCH\nBWCD\nBWCA\nIWCH\nIWCD\nIWCA\nPSCH\nPSCD\nPSCA\nWHCH\nWHCD\nWHCA\nVCCH\nVCCD\nVCCA\nMaxCH\nMaxCD\nMaxCA\nAvgCH\nAvgCD\nAvgCA\nB365C.2.5\nB365C.2.5.1\nPC.2.5\nPC.2.5.1\nMaxC.2.5\nMaxC.2.5.1\nAvgC.2.5\nAvgC.2.5.1\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\nE0\n09/08/2019\n20:00\nLiverpool\nNorwich\n4\n1\nH\n4\n0\nH\nM Oliver\n15\n12\n7\n5\n9\n9\n11\n2\n0\n2\n0\n0\n1.14\n10.00\n19.00\n1.14\n8.25\n18.50\n1.15\n8.00\n18.00\n1.15\n9.59\n18.05\n1.12\n8.5\n21.00\n1.14\n9.5\n23.00\n1.16\n10.00\n23.00\n1.14\n8.75\n19.83\n1.40\n3.00\n1.40\n3.11\n1.45\n3.11\n1.41\n2.92\n-2.25\n1.96\n1.94\n1.97\n1.95\n1.97\n2.00\n1.94\n1.94\n1.14\n9.50\n21.00\n1.14\n9.0\n20.00\n1.15\n8.00\n18.00\n1.14\n10.43\n19.63\n1.11\n9.50\n21.00\n1.14\n9.50\n23.00\n1.16\n10.50\n23.00\n1.14\n9.52\n19.18\n1.3\n3.50\n1.34\n3.44\n1.36\n3.76\n1.32\n3.43\n-2.25\n1.91\n1.99\n1.94\n1.98\n1.99\n2.07\n1.90\n1.99\n\n\nE0\n10/08/2019\n12:30\nWest Ham\nMan City\n0\n5\nA\n0\n1\nA\nM Dean\n5\n14\n3\n9\n6\n13\n1\n1\n2\n2\n0\n0\n12.00\n6.50\n1.22\n11.50\n5.75\n1.26\n11.00\n6.10\n1.25\n11.68\n6.53\n1.26\n13.00\n6.0\n1.24\n12.00\n6.5\n1.25\n13.00\n6.75\n1.29\n11.84\n6.28\n1.25\n1.44\n2.75\n1.49\n2.77\n1.51\n2.77\n1.48\n2.65\n1.75\n2.00\n1.90\n2.02\n1.90\n2.02\n1.92\n1.99\n1.89\n12.00\n7.00\n1.25\n11.00\n6.0\n1.26\n11.00\n6.10\n1.25\n11.11\n6.68\n1.27\n11.00\n6.50\n1.24\n12.00\n6.50\n1.25\n13.00\n7.00\n1.29\n11.14\n6.46\n1.26\n1.4\n3.00\n1.43\n3.03\n1.50\n3.22\n1.41\n2.91\n1.75\n1.95\n1.95\n1.96\n1.97\n2.07\n1.98\n1.97\n1.92\n\n\nE0\n10/08/2019\n15:00\nBournemouth\nSheffield United\n1\n1\nD\n0\n0\nD\nK Friend\n13\n8\n3\n3\n10\n19\n3\n4\n2\n1\n0\n0\n1.95\n3.60\n3.60\n1.95\n3.60\n3.90\n1.97\n3.55\n3.80\n2.04\n3.57\n3.90\n2.00\n3.5\n3.80\n2.00\n3.6\n4.00\n2.06\n3.65\n4.00\n2.01\n3.53\n3.83\n1.90\n1.90\n1.96\n1.96\n2.00\n1.99\n1.90\n1.93\n-0.50\n2.01\n1.89\n2.04\n1.88\n2.04\n1.91\n2.00\n1.88\n1.95\n3.70\n4.20\n1.95\n3.6\n3.90\n1.97\n3.55\n3.85\n1.98\n3.67\n4.06\n1.95\n3.60\n3.90\n2.00\n3.60\n4.00\n2.03\n3.70\n4.20\n1.98\n3.58\n3.96\n1.9\n1.90\n1.94\n1.97\n1.97\n1.98\n1.91\n1.92\n-0.50\n1.95\n1.95\n1.98\n1.95\n2.00\n1.96\n1.96\n1.92\n\n\nE0\n10/08/2019\n15:00\nBurnley\nSouthampton\n3\n0\nH\n0\n0\nD\nG Scott\n10\n11\n4\n3\n6\n12\n2\n7\n0\n0\n0\n0\n2.62\n3.20\n2.75\n2.65\n3.20\n2.75\n2.65\n3.20\n2.75\n2.71\n3.31\n2.81\n2.70\n3.2\n2.75\n2.70\n3.3\n2.80\n2.80\n3.33\n2.85\n2.68\n3.22\n2.78\n2.10\n1.72\n2.17\n1.77\n2.20\n1.78\n2.12\n1.73\n0.00\n1.92\n1.98\n1.93\n2.00\n1.94\n2.00\n1.91\n1.98\n2.70\n3.25\n2.90\n2.65\n3.1\n2.85\n2.60\n3.20\n2.85\n2.71\n3.19\n2.90\n2.62\n3.20\n2.80\n2.70\n3.25\n2.90\n2.72\n3.26\n2.95\n2.65\n3.18\n2.88\n2.1\n1.72\n2.19\n1.76\n2.25\n1.78\n2.17\n1.71\n0.00\n1.87\n2.03\n1.89\n2.03\n1.90\n2.07\n1.86\n2.02\n\n\nE0\n10/08/2019\n15:00\nCrystal Palace\nEverton\n0\n0\nD\n0\n0\nD\nJ Moss\n6\n10\n2\n3\n16\n14\n6\n2\n2\n1\n0\n1\n3.00\n3.25\n2.37\n3.20\n3.20\n2.35\n3.10\n3.20\n2.40\n3.21\n3.37\n2.39\n3.10\n3.3\n2.35\n3.20\n3.3\n2.45\n3.21\n3.40\n2.52\n3.13\n3.27\n2.40\n2.20\n1.66\n2.23\n1.74\n2.25\n1.74\n2.18\n1.70\n0.25\n1.85\n2.05\n1.88\n2.05\n1.88\n2.09\n1.84\n2.04\n3.40\n3.50\n2.25\n3.30\n3.3\n2.25\n3.40\n3.30\n2.20\n3.37\n3.45\n2.27\n3.30\n3.30\n2.25\n3.40\n3.30\n2.25\n3.55\n3.50\n2.34\n3.41\n3.37\n2.23\n2.2\n1.66\n2.22\n1.74\n2.28\n1.77\n2.17\n1.71\n0.25\n1.82\n2.08\n1.97\n1.96\n2.03\n2.08\n1.96\n1.93\n\n\nE0\n10/08/2019\n15:00\nWatford\nBrighton\n0\n3\nA\n0\n1\nA\nC Pawson\n11\n5\n3\n3\n15\n11\n5\n2\n0\n1\n0\n0\n1.90\n3.40\n4.00\n1.90\n3.40\n4.33\n1.93\n3.40\n4.25\n1.98\n3.44\n4.37\n1.95\n3.4\n4.20\n1.95\n3.5\n4.33\n2.00\n3.50\n4.60\n1.94\n3.41\n4.26\n2.10\n1.72\n2.19\n1.76\n2.24\n1.76\n2.16\n1.71\n-0.50\n1.95\n1.95\n1.98\n1.95\n1.98\n1.98\n1.94\n1.94\n2.10\n3.25\n4.20\n2.10\n3.1\n4.00\n2.05\n3.20\n4.00\n2.05\n3.38\n4.12\n2.05\n3.25\n4.00\n2.15\n3.30\n3.90\n2.15\n3.38\n4.20\n2.07\n3.27\n4.04\n2.1\n1.72\n2.16\n1.78\n2.20\n1.78\n2.14\n1.73\n-0.50\n2.04\n1.86\n2.05\n1.88\n2.12\n1.91\n2.05\n1.84"
  },
  {
    "objectID": "Euro24/comp_premier_league.html#model-comparisons",
    "href": "Euro24/comp_premier_league.html#model-comparisons",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\n\nCode\n   # Assuming df is your dataframe\n  df %&gt;% filter(type == 'NLL_PRED') %&gt;% \n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line() + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models on the Premier League 2019 dataset', \n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(2.5, 4) +\n    theme(legend.position = \"bottom\") +\n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\n\nEspecially for small training data, the hierarchical model performs better than the non-hierarchical model.\nThe Correlated Dataset model performs slightly better than non-correlated one\nThere is partically no difference in predictive performance when comparing the model with and without Cholesky decomposition.\nThe negative binomial model performs comparable to Poisson model.\nAll models start to deteriorate at around 280 training data. This is due to the interruped season in 2019/2020 due to the COVID-19 pandemic."
  },
  {
    "objectID": "Euro24/comp_premier_league.html#comparison-of-predicted-vs-psis-loo",
    "href": "Euro24/comp_premier_league.html#comparison-of-predicted-vs-psis-loo",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Comparison of predicted vs PSIS-LOO",
    "text": "Comparison of predicted vs PSIS-LOO\n\n\nCode\n  df %&gt;% filter(type %in% c('NLL_PRED', 'NLL_PSIS', 'NLL_PRED_STAN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = type)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models on the Premier League 2019 dataset', \n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(2.5, 4) +\n    facet_wrap(~name) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\n\nFor few training data, PSIS-LOO estimator is"
  },
  {
    "objectID": "Euro24/comp_premier_league.html#result-nlls",
    "href": "Euro24/comp_premier_league.html#result-nlls",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Result NLLs",
    "text": "Result NLLs\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models on the Premier League 2019 dataset', \n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area"
  },
  {
    "objectID": "Euro24/comp_premier_league.html#betting-returns",
    "href": "Euro24/comp_premier_league.html#betting-returns",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Betting Returns",
    "text": "Betting Returns\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('BET_RETURN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models on the Premier League 2019 dataset', \n      x = 'Number of training data', \n      y = 'Betting Returns'\n    ) + \n    #ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area"
  },
  {
    "objectID": "Euro24/comp_premier_league.html#technical-details",
    "href": "Euro24/comp_premier_league.html#technical-details",
    "title": "Comparison of different models on the Premier League 2019 dataset",
    "section": "Technical Details",
    "text": "Technical Details\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('MIN_SUM_PROB')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models on the Premier League 2019 dataset', \n      x = 'Number of training data', \n      y = 'Sum of Probabilities from 0 to 10 goals (should be 1)'\n    ) + \n    ylim(0.75, 1.01) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area"
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html",
    "href": "Euro24/comp_bundes_liga_2000.html",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "",
    "text": "The experiments take some time to run, therefore we used the R-Script to producte the results https://github.com/oduerr/da/blob/master/website/Euro24/eval_performance_runner.R."
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#loading-the-data",
    "href": "Euro24/comp_bundes_liga_2000.html#loading-the-data",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\n  df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_20.csv')\n  df &lt;- mutate(df, name = sub(\"^.*/\", \"\", name))\n  \n  \n  df_raw = read.csv('~/Documents/GitHub/da/website/Euro24/bundesliga2000.csv')\n  head(df_raw) %&gt;% kable()\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\nAttendance\nReferee\nHS\nAS\nHST\nAST\nHHW\nAHW\nHC\nAC\nHF\nAF\nHO\nAO\nHY\nAY\nHR\nAR\nHBP\nABP\nGBH\nGBD\nGBA\nIWH\nIWD\nIWA\nLBH\nLBD\nLBA\nSBH\nSBD\nSBA\nWHH\nWHD\nWHA\n\n\n\n\nD1\n11/08/00\nDortmund\nHansa Rostock\n1\n0\nH\n0\n0\nD\n61000\nJ&lt;f6&gt;rg Kessler\n17\n5\n7\n2\n0\n0\n7\n3\n25\n19\n4\n8\n1\n5\n0\n0\n10\n50\n1.5\n3.4\n5.0\n1.45\n3.5\n5.0\nNA\nNA\nNA\n1.50\n3.50\n6.00\n1.44\n3.6\n6.5\n\n\nD1\n12/08/00\nBayern Munich\nHertha\n4\n1\nH\n1\n0\nH\n57000\nMarkus Merk\n14\n11\n6\n5\n1\n0\n4\n9\n13\n12\n3\n3\n1\n0\n0\n0\n10\n0\n1.3\n4.5\n6.0\n1.45\n3.5\n5.0\nNA\nNA\nNA\n1.40\n3.75\n7.00\n1.44\n3.6\n6.5\n\n\nD1\n12/08/00\nFreiburg\nStuttgart\n4\n0\nH\n2\n0\nH\n22500\nHelmut Krug\n15\n18\n7\n5\n0\n0\n4\n7\n22\n17\n0\n0\n1\n1\n0\n0\n10\n10\n2.4\n3.1\n2.5\n2.30\n2.9\n2.5\nNA\nNA\nNA\n2.60\n3.25\n2.38\n2.40\n3.2\n2.5\n\n\nD1\n12/08/00\nHamburg\nMunich 1860\n2\n2\nD\n2\n2\nD\n35000\nHerbert Fandel\n18\n9\n5\n7\n1\n0\n5\n3\n0\n0\n0\n0\n2\n2\n0\n1\n20\n45\n1.8\n3.3\n3.8\n1.80\n3.0\n3.5\nNA\nNA\nNA\n1.75\n3.30\n4.00\n1.66\n3.3\n4.5\n\n\nD1\n12/08/00\nKaiserslautern\nBochum\n0\n1\nA\n0\n0\nD\n38000\nHelmut Fleischer\n11\n5\n2\n2\n0\n0\n5\n5\n9\n8\n0\n0\n1\n0\n0\n0\n10\n0\n1.5\n3.4\n4.6\n1.45\n3.5\n5.0\nNA\nNA\nNA\n1.44\n3.80\n6.00\n1.50\n3.6\n5.5\n\n\nD1\n12/08/00\nLeverkusen\nWolfsburg\n2\n0\nH\n2\n0\nH\n22500\nEdgar Steinborn\n18\n5\n5\n5\n0\n0\n6\n4\n25\n22\n0\n0\n0\n2\n0\n0\n0\n20\n1.4\n4.0\n5.5\n1.55\n3.3\n4.5\nNA\nNA\nNA\n1.44\n3.80\n6.00\n1.44\n3.6\n6.5"
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#model-comparisons",
    "href": "Euro24/comp_bundes_liga_2000.html#model-comparisons",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Model Comparisons",
    "text": "Model Comparisons\nWe use the negative log likelihood (NLL) as a measure of the predictive performance of the models. The lower the NLL, the better the model. However, strictly speaking it is the negative log posterior predictive density (divided by \\(n\\)) evaluated at the \\(n\\) games after the training data.\n\\[\n\\text{NLL} = -\\frac{1}{n}\\sum_{i=1}^n \\log p(y_i | x_i, \\theta)\n\\]\n\n\nCode\n   # Assuming df is your dataframe\n  df %&gt;% filter(type == 'NLL_PRED') %&gt;% \n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line() + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2000 dataset', \n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(2.5, 4) +\n    theme(legend.position = \"bottom\") +\n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\n\nEspecially for small training data, the hierarchical model performs better than the non-hierarchical model.\nThe Correlated Dataset model performs slightly better than non-correlated one\nThere is partically no difference in predictive performance when comparing the model with and without Cholesky decomposition.\nThe negative binomial model performs comparable to Poisson model."
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#comparison-of-predicted-vs-psis-loo",
    "href": "Euro24/comp_bundes_liga_2000.html#comparison-of-predicted-vs-psis-loo",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Comparison of predicted vs PSIS-LOO",
    "text": "Comparison of predicted vs PSIS-LOO\n\n\nCode\n  df %&gt;% filter(type %in% c('NLL_PRED', 'NLL_PSIS', 'NLL_PRED_STAN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = type)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title =  'Comparison of different models for the Bundesliga 2000 dataset',\n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(2.5, 4) +\n    facet_wrap(~name) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations"
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#nnl-for-win-draws-and-losses",
    "href": "Euro24/comp_bundes_liga_2000.html#nnl-for-win-draws-and-losses",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "NNL for win, draws and losses",
    "text": "NNL for win, draws and losses\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2000 dataset',\n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\n\nThe NLL for the bookie is always better than the NLL of the models, so we should not bet."
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#betting-returns",
    "href": "Euro24/comp_bundes_liga_2000.html#betting-returns",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Betting Returns",
    "text": "Betting Returns\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('BET_RETURN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) +  \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2000 dataset',\n      x = 'Number of training data', \n      y = 'Betting Returns'\n    ) + \n    #ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\nWe see quite some fluctuation in the betting return. Since the NLL shows that the odds from the booki are always better then the NLLs of the models we should not bet."
  },
  {
    "objectID": "Euro24/comp_bundes_liga_2000.html#technical-details",
    "href": "Euro24/comp_bundes_liga_2000.html#technical-details",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Technical Details",
    "text": "Technical Details\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('MIN_SUM_PROB')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2000 dataset',\n      x = 'Number of training data', \n      y = 'Sum of Probabilities from 0 to 10 goals (should be 1)'\n    ) + \n    ylim(0.75, 1.01) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('num_divergent')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2000 dataset',\n      x = 'Number of training data', \n      y = 'Number of Divergent Transitions (sqrt scale)'\n    ) + \n    theme(legend.position = \"bottom\") + \n    scale_y_sqrt() + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf %&gt;% \n  filter(type %in% c('ebfmi')) %&gt;%\n  ggplot(aes(x = ntrain, y = res, color = name)) + \n  geom_line(aes(linetype = name)) + \n  geom_point() + \n  theme_minimal() + \n  labs(\n    title = 'Comparison of different models for the Bundesliga 2000 dataset',\n    x = 'Number of training data', \n    y = 'ebfmi' \n  ) + \n  theme(legend.position = \"bottom\") + \n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") + \n  annotate(\"text\", x = Inf, y = 0.33, label = \"Acceptable\", hjust = 1.1, color = \"red\") + \n  annotate(\"text\", x = Inf, y = 0.27, label = \"Non-Acceptable\", hjust = 1.1, color = \"red\") +\n  coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area"
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html",
    "href": "lecture_notes/MCMC/MCMC.html",
    "title": "Lecture Notes on MCMC",
    "section": "",
    "text": "In this lecture note, we will discuss the basics of Markov Chain Monte Carlo (MCMC) methods. MCMC methods are a class of algorithms that are used to sample from a probability distribution \\(p(\\theta)\\) such as the posterior \\(p(\\theta|D) \\propto p(D|\\theta) p(\\theta)\\). While there are more advanced MCMC methods, we will focus on the Metropolis-Hastings algorithm, which is a simple and widely used MCMC algorithm and shows the principles of MCMC methods."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#a-note-on-notation",
    "href": "lecture_notes/MCMC/MCMC.html#a-note-on-notation",
    "title": "Lecture Notes on MCMC",
    "section": "üìù A note on notation",
    "text": "üìù A note on notation\nIn this lecture note, we will use the following notation:\n\n\\(\\theta\\) is a parameter of interest. This could be a scalar or a vector.\n\\(D\\) is the data.\n\\(p(\\theta)\\) is the prior distribution of \\(\\theta\\) and\n\\(p(D|\\theta)\\) is the likelihood of the data given \\(\\theta\\).\n\\(p(\\theta|D)\\) is the posterior distribution of \\(\\theta\\) given the data \\(D\\)\n\nAll expressions are read right to left, e.g.¬†\\(p(\\theta|D)\\) is the posterior distribution of \\(\\theta\\) given the data \\(D\\) and \\(P_{ij}\\) is the probability of moving from state \\(j\\) to state \\(i\\).\n\nUsing \\(p(\\theta)\\) instead of \\(p(\\theta|D)\\)\nSince the MCMC methods works with any probability distribution, we will use \\(p(\\theta)\\) to denote the target distribution that we want to sample from. In the context of Bayesian inference, \\(p(\\theta)\\) is the posterior distribution \\(p(\\theta|D)\\) without explicitly stating the data \\(D\\). You can also think of it as the posterior when you have no data (the prior).\n\n\nüìä Densities vs.¬†üìâ probabilities\nWhile in general in the lecture we will use the terms ‚Äúdensity‚Äù and ‚Äúprobability‚Äù interchangeably. However, in the context of MCMC methods, we will use the term ‚Äúdensity‚Äù to refer to the probability density function of a continuous distribution and ‚Äúprobability‚Äù to refer to the probability mass function of a discrete distribution. In case the destinction is crucial, we will use the terms \\(P(\\cdot)\\) for probabilities and \\(p(\\cdot)\\) for densities."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#a-high-level-overview-of-markov-chains.",
    "href": "lecture_notes/MCMC/MCMC.html#a-high-level-overview-of-markov-chains.",
    "title": "Lecture Notes on MCMC",
    "section": "üî≠ A high level overview of Markov Chains.",
    "text": "üî≠ A high level overview of Markov Chains.\nA Markov chain is a sequence of random variables \\(\\theta_1, \\theta_2, \\theta_3, \\ldots\\) which can be viewed as a time series where the next value depends only on the current value (Markov Property). In Figure¬†1: we show two Markov chains, starting from different initial values. Note that after about 150 steps both chains fluctuate around the same value. This is the stationary distribution of the Markov chain. A Markov chain reaches a stationary distribution when the distribution of the states no longer changes as the chain progresses.\n\n\n\n\n\n\nFigure¬†1: Shown are two Markov chains for the scalar quantity $\\theta$. The chains start from different initial positions. After about 150 time steps the chains fluctate around the same value.\n\n\n\nIn Figure¬†2, we show a histogram1 of the values of the two chains after from 250 steps onward. The histogram indicate that the values of the chains are distributed around the same distribution is the stationary distribution of the Markov chain.\n\n\n\n\n\n\nFigure¬†2: The Histogram (actually density estimates) for of the two chains after 250 steps\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe key idea of MCMC methods is to construct a Markov chain that has a stationary distribution equal to the target distribution \\(p(\\theta)\\). Once the Markov chain has converged to the stationary distribution, we can use the samples from the chain to approximate the target distribution.\n\n\nSo how do we construct a Markov chain with a stationary distribution equal to the target distribution \\(p(\\theta)\\)? To do so, we draw insights from statistical physics.\n\nStatistical physics and detailed balance\nConsider a glass of water in which we add a drop of ink. The ink will spread out in the water until it is uniformly distributed. This is an example of a system that has reached equilibrium. The the following ideas, are important inspirations for MCMC methods:\nErgodic Hypothesis - The diffusion process can be viewed as either a single particle or many particles. Initially concentrated in one region, a particle will eventually explore the entire space uniformly, regardless of its starting point. This idea, known as the Ergodic Hypothesis, was first proposed by Boltzmann.\nThe reaching of the stationary distribution - Over time, particles reach a stationary distribution (in physics is referred to as the equilibrium distribution). For example, ink in water eventually reaches a uniform distribution. However, not all distributions are uniform. For instance, the barometric height equation describes how particles in the air are more concentrated at the ground and decrease exponentially with height.\nDetailed Balance - At stationarity, the rate at which particles move from one point to another is equal to the rate at which they move in the opposite direction. This concept is known as detailed balance and is crucial for understanding the Metropolis-Hastings algorithm.\nFrom that, we can design the MCMC Algorithms algorithms. Roughly as follows * Start with parameter of interest \\(\\theta\\) at certain position * Mimics the diffusion process but with a stationary distribution of our choosing \\(p(\\theta)\\) * Algorithm needs to obey the detailed balance condition, this allow us to define the transition probabiliy with which we go from one value to another."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#detailed-balance",
    "href": "lecture_notes/MCMC/MCMC.html#detailed-balance",
    "title": "Lecture Notes on MCMC",
    "section": "‚öñÔ∏è Detailed balance",
    "text": "‚öñÔ∏è Detailed balance\nFor a Markov chain to be useful, it needs to convert to a stationary distribution and the stationary distribution needs to be the target distribution \\(p(\\theta)\\). The Metropolis-Hastings algorithm is a simple MCMC algorithm that ensures that the Markov chain converges to the desired stationary distribution. The Metropolis-Hastings algorithm is based on the principle of detailed balance. Also almost all other MCMC algorithms (such as Hamiltonian Monte Carlo, Slice Sampling, and Gibbs Sampling) are based on the principle of detailed balance. So what is detailed balance?\nFor the ink particles, we have a uniforn distribution as stationary distribution. For the ink particles in the water glass the detailed balance condition is that the rate at which particles move from one point to another is equal to the rate at which they move in the opposite direction. Remember the example with the king visiting the islands? The king will visit the islands with a probability proportional to the size on the island. So the stationary distribution is not uniform anymore. The detailed balance condition can be used to determine the probability \\(P_{ji}\\) of moving from island \\(i\\) to island \\(j\\). Now consider N Kings, in order that the number of Kings on the islands have reached a stationary distribution, at each time step as many Kings need to move from one island to another as move in the opposite direction. Otherwise the number of Kings on the islands would change over time and we have not yet reached the stationary distribution. The detailed balance condition is (a bit slopy) given by:\n\\[\n  P_{ij} N_j = P_{ji} N_i\n\\]\nwith \\(P_{ij}\\) the probability of moving from island \\(i\\) to island \\(j\\) and \\(N_i\\) the number of Kings on island \\(i\\). Let‚Äôs divide both sides by \\(N\\)\n\\[\nP_{ij}\\underbrace{P_{j}}_{N_j/N} = P_{ji}\\underbrace{P_{i}}_{N_j/N}\n\\]\nFigure¬†3 shows the detailed balance situation in a system with discrete states (such as kings). Note that the detailed balence is between two states \\(i\\) and \\(j\\) and that other states \\(k\\) are not taken into account.\n\n\n\n\n\n\nFigure¬†3: Illustration of the detailed balance condition in a Markov chain. The transition probabilities $P_{ij}$ and $P_{ji}$ between states $j$ and $i$ are shown, along with their respective stationary probabilities $P_i$ and $P_j$. The light gray dashed arrows indicate the presence of other states in the Markov chain, emphasizing that the detailed balance condition is specifically applied between states $i$ and $j$\n\n\n\nThe detailed balance condition the probability \\(P_{ij}P_j\\) (fraction of Kings/Particle) move from \\(j\\) to \\(i\\) is equal to the probability \\(P_{ji}P_i\\) (fraction of Kings/Particle) move from \\(i\\) to \\(j\\).\n\nüéõÔ∏èControling \\(P_{ij}\\)\nLet‚Äôs choose \\(P_{ij}\\) in our favor, so that in equilibrium the distribution \\(P_i\\) is the distribution we want. We can do this by proposing a move from \\(i\\) to \\(j\\) with a probability \\(T_{ij}\\) and then accept the move with an acceptance probability \\(A_{ij}\\). The cool idea that physicists had in the 1950‚Äôs is to choose the acceptence probability \\(A_{ij}\\) as:\n\nMetropolis-Hastings acceptance probability\n\\[\nA_{ij} = \\min (1, \\frac{T_{ji}P_i}{T_{ij} P_j})\n\\]\nWhy is detailed balance fullfilled?\n\nIf we know move a particle according to the Metropolis Hastings Acceptance rate. We start in state \\(\\theta_0\\) and propose a move to \\(\\theta^*\\) with \\(T(\\theta^*|\\theta_0)\\). We accept the new state with a probability according to the MH Acceptance probability. If we repeat we get the following chain of moves.\n\nInitialize: Start with an initial value \\(\\theta_0\\).\nPropose: new state \\(\\theta^*\\) from old state \\(\\theta_t\\) with a proposal distribution \\(T(\\theta^* | \\theta_t)\\).\nCalculate Acceptance Probability: Compute the acceptance probability \\[\n  A  = \\min \\left(1, \\frac{T(\\theta_t | \\theta^*) p(\\theta^*)}{T(\\theta^* | \\theta_t) p(\\theta_t)}\\right)\n\\]\nAccept proposed state: With probability \\(A\\), set \\(\\theta_{t+1} = \\theta^*\\). Otherwise, continue with old state, \\(\\theta_{t+1} = \\theta_t\\).\nIterate: Repeat steps 2-4 for a large number of iterations to ensure convergence to the stationary distribution.\n\n\n\nA short note on the continuous case\nSo far we have discussed the Metropolis-Hastings algorithm in the context of discrete states. However, the Metropolis-Hastings algorithm can be extended to continuous states, easily. There might be pitfalls when changing to the continuous case, don‚Äôt just replace the probabilities \\(P_i\\) with densities \\(p(\\theta)\\)! However here the Metropolis-Hastings algorithm is the same as in the discrete case. Just exchange the probabilities with densities.\n\n\n\n\n\n\nNote\n\n\n\nFor the acceptance rate \\(p(\\theta)\\) is needed only up to a constant factor, this makes it ideal for Bayesian inference, where we just need \\(p(\\theta|D) \\propto p(D|\\theta)p(\\theta)\\).\n\n\nThis process ensures that the Markov chain will converge to the target distribution \\(p(\\theta)\\), allowing us to approximate the distribution through the samples obtained from the chain. Let‚Äôs give it a try, we assume symetric proposal distribution \\(T(\\theta^*|\\theta) = T(\\theta|\\theta^*)\\) and we want to sample from the \\(p(\\theta) \\propto Exp(\\lambda = 1/10)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nüìù Exercise (Simple MCMC Algorithm)\n\n\n\n\n\n\nüìù Exercise (Simple MCMC Algorithm)\n\n\n\nPlay around with the code above.\n\nWhat do you observe if you change the standard deviation of the proposal distribution?\nChange the target distribution to \\(p(\\theta) \\propto \\exp(-\\theta^2/2)\\)."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#the-hairy-caterpillar",
    "href": "lecture_notes/MCMC/MCMC.html#the-hairy-caterpillar",
    "title": "Lecture Notes on MCMC",
    "section": "üêõThe hairy caterpillar",
    "text": "üêõThe hairy caterpillar\nThere can be several problems when using the Metropolis-Hastings algorithm.\n\nSlow mixing: The Markov chain takes a long time to converge to the stationary distribution.\nRandom walk behavior: Too small steps in the proposal distribution can lead to a behavior, where the chain moves slowly through the parameter space and many samples are correlated.\nTrap in local minima: The chain gets trapped in a local minimum and does not explore the parameter space properly\n\n\nüõ†Ô∏è Tools to inspect the chain\nFirst look at the trace of the chain(s), this is called traceplot. Simply plot the parameter values against the steps. Such a traceplot has been shown in Figure¬†1. We clearly see that the 2 chains have converged to the same region (stationary distribution). Another traceplot is shown in Figure¬†4.\n\n\n\n\n\n\nFigure¬†4: Traceplot of the parameter sigma inagainst the steps. While the Metropolis Algorithm provides valid samples, the samples are highly correlated compared to the more advanced Stan algorithm\n\n\n\nThis traceplot compares the parameter \\(a\\) sampled with Stan (a more advanced MCMC algorithm) and the Metropolis-Hastings algorithm. We see that the Metropolis-Hastings algorithm moves slowly through the parameter space. In contrast, the Stan samples are less correlated and move more faster through the complete parameter space. This fast moving through the parameter space is called ‚Äúefficient mixing‚Äù or a ‚Äúhairy caterpillar‚Äù. The effect of this ‚Äúhairy caterpillar‚Äù is that the samples are less correlated and hence provide more information about the posterior distribution of the parameter. This can be quantified by the effective sample size (ESS), which is the number of independent samples that provide the same information as the correlated samples.\nFinally, in the case of bimodal distributions, MCMC algorithms can get stuck in one of the modes. This is shown in Figure¬†5 . Some chains of have converged to the one mode and it takes a long time to switch to the other mode. While the true bimodal distribution would be sampled for infinite times it practically takes a long time to sample from both modes and the respecitve modes are not sampled equally.\n\n\n\n\n\n\nFigure¬†5: Trace Plot of Samples from a Bimodal Distribution, all chains sample the repective mode quite successfully but stay too long in a mode before jumping to the next.\n\n\n\nYou can play around a bit with the overlap of the two modes and the standard deviation of the proposal distribution in https://oduerr.github.io/anim/mcmc_mh.html"
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#continuous-case-details",
    "href": "lecture_notes/MCMC/MCMC.html#continuous-case-details",
    "title": "Lecture Notes on MCMC",
    "section": "ü§î Continuous case details*",
    "text": "ü§î Continuous case details*\n*This is and advanced topic, which might be well skipped on first reading.\nLet‚Äôs look at the continous case in more detail. We derived the Metropolis-Hastings algorithm for the discrete case, where we have probabilities and not densities. The MH acceptance probability is given by\n\\[\nA_{ij} = \\min (1, \\frac{T_{ji}P_i}{T_{ij} P_j})\n\\]\nThe probability \\(P_i\\) for the state \\(\\theta_i\\) is replaced by the density \\(p(\\theta_i)\\) times the infinitisimal volume element \\(d\\theta_i\\). The transition probability \\(T_{ji}\\) that starting in \\(i\\) we move to \\(j\\) is replaced by the proposal density \\(T(\\theta_j|\\theta_i)\\) times the infinitisimal volume element \\(d\\theta_j\\). Note we have a probability in the target volumne \\(d\\theta_j\\) and not in the ‚Äúfrom‚Äù volume \\(d\\theta_i\\). So altogether we get\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{d\\theta_j T(\\theta_j|\\theta_i) p(\\theta_i) d\\theta_i}{d\\theta_i T(\\theta_i|\\theta_j) p(\\theta_j) d\\theta_j}\\right)\n\\] Note that the volume elements \\(d\\theta_i\\) and \\(d\\theta_j\\) cancel out. So we get\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{T(\\theta_j|\\theta_i) p(\\theta_i)}{T(\\theta_i|\\theta_j) p(\\theta_j) }\\right)\n\\tag{1}\\] Which is the same as in the discrete case! However, there might be pitfalls when changing to the continuous case, don‚Äôt just replace the probabilities \\(P_i\\) with densities \\(p(\\theta)\\)! This is for example the case in the following.\n\n\n\n\n\n\nNote\n\n\n\nThe MH Criterion is the same in the continuous case as in the discrete case. Just replace the probabilities with densities.\n\n\n\n‚ö†Ô∏è Sampling in a different space\nThere are several reasons, to sample in a different space, then the space where the target distribution \\(p(\\theta)\\) is defined. It might be hard to sample in \\(\\theta\\), but it‚Äôs easy to sample in another space \\(x\\). For example in \\(\\theta\\) the probability landscape might have very narrow regions, but in \\(x\\) it‚Äôs much nicer behaved. We will encounter this in the famous Neal‚Äôs funnel example later. Another reason is that \\(\\theta\\) is in a restricted space. We will use the following example in spherical coordinates to illustrate this. Suppose we have a problem where we have a good idea that the radius \\(r\\) is around 1, but we have no idea about the angle \\(\\varphi\\). We can express this in the following prior\n\\[\np(\\theta) = p(r,\\varphi) = p(r)p(\\varphi) = N(r|1,0.1) \\cdot U(\\varphi|0,2\\pi)\n\\] Assume, we have no data, then the Target Distribution is then given by \\(p(r,\\varphi) \\propto N(r|1,0.1)\\). Note that in this space, we have restrictions on the parameters \\(r &gt; 0\\) and \\(0 \\le \\varphi &lt; 2\\pi\\). We have to design the proposal density \\(T(\\theta^*|\\theta)\\) such that these restrictions are intact. While this is possible in the Metropolis-Hastings algorithm, it‚Äôs really hard to enforce this in more advanced algorithms like Hamiltonian Monte Carlo, where we need an unrestricted space. To solve this we transform the problem in a difference space \\(x\\), where we have no restrictions. The acceptance probability is then Equation¬†1 with \\(\\theta\\) replaced by \\(x\\).\n\\[\nA(x_i \\leftarrow x_j) = \\min \\left(1, \\frac{T(x_j|x_i) p(x_i)}{T(x_i|x_j) p(x_j) }\\right)\n\\tag{2}\\]\nSo what is missing is \\(p(x)\\). Be careful and consider probabilities:\n\\[\n  p(x) dx = p(\\theta) d\\theta \\quad \\Rightarrow \\quad p(x) = p(\\theta) \\left| \\frac{d \\theta}{d x} \\right|\n\\] With the Jacobian Determinant \\(J = \\left| \\frac{d\\theta}{dx} \\right|\\), e.g. Equation¬†1 becomes:\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{T(\\theta_j|\\theta_i) p(\\theta_i)}{T(\\theta_i|\\theta_j) p(\\theta_j)} \\right) = \\min \\left(1, \\frac{T(x_j|x_i) p(x_i) J_i}{T(x_i|x_j) p(x_j) J_j} \\right)\n\\]\nWe have \\(x_1 = r \\cos(\\varphi)\\) and \\(x_2 = r \\sin(\\varphi)\\), so calculation of \\(\\frac{d x}{d \\theta}\\) would be simple, so we calculate the inverse of the Jacobian Determinant: \\[\nJ^{-1} =\n\\left|\n\\begin{pmatrix}\n\\frac{\\partial x_1}{\\partial r} & \\frac{\\partial x_1}{\\partial \\varphi} \\\\\n\\frac{\\partial x_2}{\\partial r} & \\frac{\\partial x_2}{\\partial \\varphi}\n\\end{pmatrix}\n\\right|\n=\n\\left|\n\\begin{pmatrix}\n\\cos(\\varphi) & -r \\sin(\\varphi) \\\\\n\\sin(\\varphi) & r \\cos(\\varphi)\n\\end{pmatrix}\n\\right| = r (\\cos^2(\\varphi) + \\sin^2(\\varphi)) = r\n\\] The Jacobian Determinant is then \\(J = 1/r\\). We can now sample in the unrestricted space \\(x_1\\) and \\(x_2\\) and apply the Jacobian Determinant. The following code snippet shows how to sample from the target distribution in unrestricted space \\(x_1\\) and \\(x_2\\) and how to correctly apply the Jacobian Determinant.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n‚ö† Likelihood in different space\nConsider the where are in a parameter space \\(\\theta\\) and we have in that space the likelihood \\(p_\\theta(D|\\theta)\\) and prior \\(p_\\theta(\\theta))\\) and thus the posterior is\n\\[\np_\\theta(\\theta|D) \\propto p_\\theta(D|\\theta) p_\\theta(\\theta)\n\\]\nWe now transform in an different space \\(\\varphi\\) with the transformation \\(\\theta = f(\\varphi)\\). The likelihood in the new space is then given by \\(p_\\varphi(D|\\varphi)\\) and the prior by \\(p_\\varphi(\\varphi)\\) and hence the posterior is\n\\[\np_\\varphi(\\varphi|D) \\propto p_\\varphi(D|\\varphi) p_\\varphi(\\varphi)\n\\]\nOf course we can sample, evaluate the likelihood and prior in either space and get the same result. What happens if we sample in the \\(\\varphi\\) space\n\nStart with \\(\\varphi_0\\)\nPropose \\(\\varphi^*\\) from \\(\\varphi_0\\) with \\(T(\\varphi^*|\\varphi_0)\\)\nCalculate the unnormalized posterior in the \\(\\varphi\\) space \\(p_\\varphi(D|\\varphi^*) p_\\varphi(\\varphi^*)\\)\nAssuming a symetric proposal distribution \\(T(\\varphi^*|\\varphi_0) = T(\\varphi_0|\\varphi^*)\\) we get the Metropolis-Hastings acceptance probability from the ratio \\(p_\\varphi(D|\\varphi^*) p_\\varphi(\\varphi^*)/p_\\varphi(D|\\varphi_0) p_\\varphi(\\varphi_0)\\).\n\nIs it OK to calculate the likelihood in the \\(\\theta\\) space by the transformed \\(\\varphi\\)? For that the ratio of the likelihoods \\(p_\\theta(D|f(\\varphi^*)) /p_\\theta(D|f(\\varphi_0)) \\overbrace{=}^{!}  p_\\varphi(D|\\varphi^*) /p_\\varphi(D|\\varphi_0)\\) needs to be the same.\nLet‚Äôs make a concrete Example."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#footnotes",
    "href": "lecture_notes/MCMC/MCMC.html#footnotes",
    "title": "Lecture Notes on MCMC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell actually its a density plot plot(density(.)) and not hist(.)‚Ü©Ô∏é"
  },
  {
    "objectID": "Euro24/euro24.html",
    "href": "Euro24/euro24.html",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "",
    "text": "The Euro 2024 ‚öΩ is a nice showcase of Bayesian Statistics. In Bayesian statistics, probabilities are seen as a degree of belief, which fits well with the nature of football. Almost everyone has beliefs about the strengths and weaknesses of the teams before seeing any games (based on historical data) and then updates these beliefs as new data comes in (games have been played)."
  },
  {
    "objectID": "Euro24/euro24.html#prediction-based-on-the-historical-data",
    "href": "Euro24/euro24.html#prediction-based-on-the-historical-data",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "Prediction based on the Historical Data",
    "text": "Prediction based on the Historical Data\nWe load the matches prior to the Euro 2024, see https://github.com/oduerr/da/blob/master/stan/Euro24/Euro_Data.md how to get the data. Note the limitations of using historic; for example, Germany only started finding their form shortly before Euro 2024."
  },
  {
    "objectID": "Euro24/euro24.html#loading-the-data",
    "href": "Euro24/euro24.html#loading-the-data",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "Loading the data",
    "text": "Loading the data\nLoaded 116 games with 24 teams.\n\n\n\n\n\n\nExpand To Learn How to set up the data for Stan\n\n\n\n\n\nPreparing the data for Stan\n\n\nCode\n#Some R-Magic to convert the team names to numbers, no need to understand this\nng = nrow(data)\nteams = unique(data$Home)\nnt = length(teams)\nht = unlist(sapply(1:ng, function(g) which(teams == data$Home[g])))\nat = unlist(sapply(1:ng, function(g) which(teams == data$Away[g])))\n\nnp=1 #Number games leaving out for prediction\nngob = ng-np #ngames obsered ngob = number of games to fit\n#print(paste0(\"Using the first \", ngob, \" games to fit the model and \", np, \" games to predict.\", \"Num teams \",  length(teams)))\nmy_data = list(\n  nt = nt, \n  ng = ngob,\n  ht = ht[1:ngob], \n  at = at[1:ngob], \n  s1 = data$score1[1:ngob],\n  s2 = data$score2[1:ngob],\n  np = np,\n  htnew = ht[(ngob+1):ng],\n  atnew = at[(ngob+1):ng],\n  s1new = data$score1[(ngob+1):ng],\n  s2new = data$score2[(ngob+1):ng]\n)\n\n\nUsing the first 115 games to fit the model and 1 games to predict. In total we have 24 teams."
  },
  {
    "objectID": "Euro24/euro24.html#preparing-the-data-for-stan",
    "href": "Euro24/euro24.html#preparing-the-data-for-stan",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "Preparing the data for Stan",
    "text": "Preparing the data for Stan\n\n\nCode\n#Some R-Magic to convert the team names to numbers, no need to understand this\nng = nrow(data)\nteams = unique(data$Home)\nnt = length(teams)\nht = unlist(sapply(1:ng, function(g) which(teams == data$Home[g])))\nat = unlist(sapply(1:ng, function(g) which(teams == data$Away[g])))\n\nnp=1 #Number games leaving out for prediction\nngob = ng-np #ngames obsered ngob = number of games to fit\n#print(paste0(\"Using the first \", ngob, \" games to fit the model and \", np, \" games to predict.\", \"Num teams \",  length(teams)))\nmy_data = list(\n  nt = nt, \n  ng = ngob,\n  ht = ht[1:ngob], \n  at = at[1:ngob], \n  s1 = data$score1[1:ngob],\n  s2 = data$score2[1:ngob],\n  np = np,\n  htnew = ht[(ngob+1):ng],\n  atnew = at[(ngob+1):ng],\n  s1new = data$score1[(ngob+1):ng],\n  s2new = data$score2[(ngob+1):ng]\n)\n\n\nUsing the first 115 games to fit the model and 1 games to predict. In total we have 24 teams."
  },
  {
    "objectID": "Euro24/euro24.html#a-model-for-the-goals-scored",
    "href": "Euro24/euro24.html#a-model-for-the-goals-scored",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "A Model for the goals scored ü•Ö",
    "text": "A Model for the goals scored ü•Ö\nWe will assume that the number of goals scored by the home team \\(s_1\\) and the away team \\(s_2\\) follows a Poisson distribution. This has been shown to be a good model for the number of goals scored in a football match. We model the rate parameter \\(\\theta\\) of the Poisson distribution, related to the attack and defense strengths of the teams, as follows:\n\\[\n    s_1 \\sim \\text{Pois}(\\theta_1) \\quad\\text{goals scored by the home team}\n\\]\n\\[\n    s_2 \\sim \\text{Pois}(\\theta_2) \\quad\\text{goals scored by the away team}\n\\]\nThis is equivalent to performing two separate Poisson regressions, one for each team.\nWe assume that:\n\\[\n    \\theta_1 = \\exp(\\text{home} + \\text{att}_\\text{ht} - \\text{def}_\\text{at})\n\\]\n\\[\n    \\theta_2 = \\exp(\\text{att}_\\text{at} - \\text{def}_\\text{ht})\n\\]\nSince there is no home advantage in the Euro (except for Germany), we set \\(\\text{home} = 0\\).\n\nPrior for the attack and defence strength\nIn Bayesian statistics, we further need to specify a prior for the parameters (our degree of believe in the attack and defense abilities before seeing any data). For that we use a hierarchical model with correlated parameters. Other models are investigated at comp_premier_league for the English Premier League 2019/2020 season and for the German Bundesliga 2000 and 2024 where the hierarchical model have been especially successful. The model is adopted from the blog_post and the paper. We extend the model to include a correlation between the attack and defense strength of the teams, since it is quite reasonable that a team that scores many goals (is above average in offense) is also good in defense."
  },
  {
    "objectID": "Euro24/euro24.html#conditioning-on-the-data-fitting-the-model",
    "href": "Euro24/euro24.html#conditioning-on-the-data-fitting-the-model",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "Conditioning on the data / Fitting the model",
    "text": "Conditioning on the data / Fitting the model\nAfter we state the model we fit the model to the data, or in Bayesian parlance, we update our degree of belief after seeing the data.\n\n\n\n\n\n\nDetails of the model MCMC sampling with Stan\n\n\n\n\n\nThe model is written in the probabilistic programming language Stan and can be found at https://github.com/oduerr/da/blob/master/website/Euro24/hier_model_cor.stan. This model used a Cholesky decomposition to model the correlation between the attack and defense strength of the teams. While this produces very effective sampling and is numerically stable, the Cholesky decomposition adds another layer of complexity to the model. We also provide a model without the Cholesky decomposition at https://github.com/oduerr/da/blob/master/website/Euro24/hier_model_cor_nocholesky.stan which is easier to understand but which is, besides the numerical difficulties, equivalent to the model with the Cholesky decomposition.\n\n\nCode\nlibrary(cmdstanr)\noptions(mc.cores = parallel::detectCores())\nhmodel &lt;- cmdstan_model('~/Documents/GitHub/da/website/Euro24/hier_model_cor.stan')\n#hmodel &lt;- cmdstan_model('~/Documents/GitHub/da/website/Euro24/hier_model_cor_nocholsky.stan')\nhfit = hmodel$sample(data = my_data)\n## Running MCMC with 4 chains, at most 10 in parallel...\n## \n## Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n## Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n## Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n## Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n## Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n## Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n## Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n## Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n## Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n## Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n## Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n## Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n## Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n## Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n## Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n## Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n## Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n## Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n## Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \n## Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \n## Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n## Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n## Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \n## Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \n## Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n## Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n## Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \n## Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \n## Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \n## Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n## Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n## Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \n## Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \n## Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \n## Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \n## Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n## Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n## Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \n## Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \n## Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n## Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n## Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n## Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \n## Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n## Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n## Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n## Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \n## Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \n## Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n## Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n## Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n## Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n## Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n## Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n## Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n## Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n## Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n## Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n## Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n## Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n## Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n## Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n## Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n## Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n## Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n## Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n## Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n## Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n## Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n## Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n## Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n## Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n## Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n## Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n## Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n## Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n## Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n## Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n## Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n## Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n## Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n## Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n## Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n## Chain 4 finished in 0.7 seconds.\n## Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n## Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n## Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n## Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n## Chain 2 finished in 0.8 seconds.\n## Chain 3 finished in 0.8 seconds.\n## Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n## Chain 1 finished in 0.9 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.8 seconds.\n## Total execution time: 1.1 seconds.\np1 = bayesplot::mcmc_rhat_hist(bayesplot::rhat(hfit))\np2 = bayesplot::mcmc_neff_hist(bayesplot::neff_ratio(hfit))\nggpubr::ggarrange(p1, p2, ncol=2)\n\n\n\n\n\n\n\n\n\nThe fitting of the model is good, as the Rhat values are close to 1 and we have no divergent transitions. The effective sample size is also good."
  },
  {
    "objectID": "Euro24/euro24.html#the-fitted-model",
    "href": "Euro24/euro24.html#the-fitted-model",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "The fitted model",
    "text": "The fitted model\nWe plot the means of the attack and defense strengths of the teams. Shown are the mean values along with the 25% and 75% quantiles. There is considerable uncertainty in the strengths of the teams, but that‚Äôs the nature of the game.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidybayes)\n\n# Step 1: Gather draws and calculate summary statistics with credible intervals\nd = hfit %&gt;%\n  tidybayes::gather_draws(A[i, j]) %&gt;%\n  group_by(i, j) %&gt;%\n  summarise(\n    average_value = mean(.value),\n    lower = quantile(.value, 0.25),  # Lower bound \n    upper = quantile(.value, 0.75),  # Upper bound \n    .groups = \"drop\"\n  )\n\n# Step 2: Create a matrix of the average values\nA = xtabs(average_value ~ i + j, data = d)\n\n# Step 3: Plot the average values\nplot(A[1,], A[2,], pch=20, xlab='Attack', ylab='Defence', main='Attack vs Defence')\n\n# Step 4: Add team labels\ntext(A[1,], A[2,], labels=teams, cex=0.7, adj=c(-0.05, -0.8))\n\n# Step 5: Add error bars for 66% credibility intervals\n# Reshape data for plotting\nd_wide &lt;- d %&gt;% spread(key = j, value = average_value)\nd_lower &lt;- d %&gt;% spread(key = j, value = lower)\nd_upper &lt;- d %&gt;% spread(key = j, value = upper)\n\n# Convert to matrices for easier plotting\nA_lower &lt;- xtabs(lower ~ i + j, data = d)\nA_upper &lt;- xtabs(upper ~ i + j, data = d)\n\n# Plot vertical error bars\narrows(A[1,], A_lower[2,], A[1,], A_upper[2,], angle=90, code=3, length=0.05, col=\"lightblue\", alpha=0.5)\n\n# Plot horizontal error bars\narrows(A_lower[1,], A[2,], A_upper[1,], A[2,], angle=90, code=3, length=0.05, col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed explanations for a single match Germany vs Scotland\n\n\n\n\n\nThe opening game of Euro 2024 was Germany vs.¬†Scotland. In the plots below, we show the posterior probabilities for the attack and defense strengths of the teams.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(tidybayes)\n\n# Function to calculate probabilities for a pairing\nid1 &lt;- which(teams == 'Germany')\nid2 &lt;- which(teams == 'Scotland')\n\n# Extract posterior distributions for Attack and Defense\nattack_germany &lt;- hfit %&gt;% tidybayes::spread_draws(A[i, j]) %&gt;% filter(j == id1, i == 1) %&gt;% select(A)\nattack_scotland &lt;- hfit %&gt;% tidybayes::spread_draws(A[i, j]) %&gt;% filter(j == id2, i == 1) %&gt;% select(A)\ndefense_germany &lt;- hfit %&gt;% tidybayes::spread_draws(A[i, j]) %&gt;% filter(j == id1, i == 2) %&gt;% select(A)\ndefense_scotland &lt;- hfit %&gt;% tidybayes::spread_draws(A[i, j]) %&gt;% filter(j == id2, i == 2) %&gt;% select(A)\n\n# Combine data into a tidy data frame\ntidy_df &lt;- bind_rows(\n  attack_germany %&gt;% mutate(Statistic = \"Attack\", Country = \"Germany\"),\n  attack_scotland %&gt;% mutate(Statistic = \"Attack\", Country = \"Scotland\"),\n  defense_germany %&gt;% mutate(Statistic = \"Defense\", Country = \"Germany\"),\n  defense_scotland %&gt;% mutate(Statistic = \"Defense\", Country = \"Scotland\")\n)\n\n# Include the mean values in the plot\nggplot(tidy_df, aes(x = A, fill = Country)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ Statistic, scales = \"free\") +\n  labs(title = \"Posterior Distributions of Attack and Defense Strengths\", x = \"Strength\", y = \"Density\") +\n  geom_vline(data = tidy_df %&gt;% group_by(Statistic, Country) %&gt;% summarise(mean_A = mean(A)), \n             aes(xintercept = mean_A, color = Country), linetype = \"dashed\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nMaking predictions\nWe can now make predictions for the game Germany vs Scotland. Below are the first samples of the posterior distribution for the attack and defense strengths of germany and scotland.\n\n\nCode\n# Extract first five samples for demonstration\ndf = data.frame(attack_germany = attack_germany$A, defense_germany = defense_germany$A, attack_scotland = attack_scotland$A, defense_scotland = defense_scotland$A) %&gt;% head() \nknitr::kable(df)\n\n\n\n\n\n\nattack_germany\ndefense_germany\nattack_scotland\ndefense_scotland\n\n\n\n\n-0.0442763\n-0.0242716\n0.1630760\n-0.1346680\n\n\n0.3273970\n-0.0198311\n-0.0108560\n0.0377780\n\n\n0.2607110\n-0.1016880\n-0.2155270\n-0.4300470\n\n\n-0.0570626\n-0.0008690\n0.0580947\n0.0156115\n\n\n0.0885649\n0.0009105\n0.1246280\n0.0193857\n\n\n0.4273620\n-0.0032764\n0.1563150\n0.0513863\n\n\n\n\n\n\n\n\nWe use the samples for posterior row by row to sample the number of goals for Germany and Scotland. We can use the samples to calculate the probability of a win, draw or loss for Germany.\n\n\nProbabilities for win/draw/loss\n\n\nCode\n set.seed(42)\n theta_germany = exp(attack_germany$A - defense_scotland$A)\n theta_scotland = exp(attack_scotland$A - defense_germany$A)\n g_germany = rpois(length(theta_germany), theta_germany)\n g_scotland = rpois(length(theta_scotland), theta_scotland)\n \n \n \n # Alternative way to calculate the probabilities\n calc_prob &lt;- function(observed, theta) {\n     mean(dpois(observed, theta))\n }\n \n plot(table(g_germany)/length(g_germany), main='Germany Goals', xlab='Goals', ylab='Probability')\n prob_goals_germany = apply(matrix(0:10, ncol=1), 1, function(x) calc_prob(x, theta_germany))\n points(0:5, prob_goals_germany[1:6])\n\n\n\n\n\n\n\n\n\nCode\n prob_goals_scotland = apply(matrix(0:10, ncol=1), 1, function(x) calc_prob(x, theta_scotland))\n plot(table(g_scotland)/length(g_scotland), main='Scotland Goals', xlab='Goals', ylab='Probability')\n points(0:5, prob_goals_scotland[1:6])\n\n\n\n\n\n\n\n\n\nCode\n prob_goals = outer(prob_goals_germany, prob_goals_scotland, '*')\n sum(prob_goals) #Should be very close to 1\n\n\n[1] 0.9999955\n\n\nCode\n print(paste0('Germany wins (simu) ',\n              mean(g_germany &gt; g_scotland), #Probability of Germany winning\n              ' probs',\n              round(sum(prob_goals[lower.tri(prob_goals, diag = FALSE)]),4)))\n\n\n[1] \"Germany wins (simu) 0.4675 probs0.4613\"\n\n\nCode\n mean(g_germany &lt; g_scotland) #Probability of Scotland winning\n\n\n[1] 0.28125\n\n\nCode\n mean(g_germany == g_scotland) #Probability of a draw\n\n\n[1] 0.25125\n\n\nCode\n print(paste0('Draw (simu) ',\n              mean(g_germany == g_scotland), #Probability of a draw\n              ' probs',\n              sum(diag(prob_goals))))\n\n\n[1] \"Draw (simu) 0.25125 probs0.259600606460745\"\n\n\nCode\n print(paste0('Scotland wins (simu) ',\n              mean(g_germany &lt; g_scotland), #Probability of Germany winning\n              ' probs',\n              round(sum(prob_goals[upper.tri(prob_goals, diag = FALSE)]),4)))\n\n\n[1] \"Scotland wins (simu) 0.28125 probs0.2791\"\n\n\nAnother way to look at is is at the joint distribution of the goals scored\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Define the function\nplot_goal_probabilities &lt;- function(attack_team1, defense_team1, attack_team2, defense_team2, team1_name = \"Team 1\", team2_name = \"Team 2\") {\n  set.seed(42)\n  \n  # Simulate goals scored using Poisson distribution\n  theta_team1 &lt;- exp(attack_team1 - defense_team2)\n  theta_team2 &lt;- exp(attack_team2 - defense_team1)\n  #g_team1 &lt;- rpois(length(theta_team1), theta_team1)\n  #g_team2 &lt;- rpois(length(theta_team2), theta_team2)\n  \n  # Calculate joint probabilities\n  #joint_prob &lt;- table(g_team1, g_team2) / length(g_team1)\n  \n  prob_g1= apply(matrix(0:10, ncol=1), 1, function(x) calc_prob(x, theta_team1))\n  prob_g2= apply(matrix(0:10, ncol=1), 1, function(x) calc_prob(x, theta_team2))\n  \n  df_joint = outer(prob_g1, prob_g2, '*') %&gt;% as.matrix\n  df_joint &lt;- reshape2::melt(df_joint, varnames = c(\"g_team1\", \"g_team2\"), value.name = \"Freq\")\n  #df_joint &lt;- as.data.frame(as.table(joint_prob))\n  colnames(df_joint) &lt;- c(\"Goals_Team1\", \"Goals_Team2\", \"Probability\")\n  df_joint$Goals_Team1 = df_joint$Goals_Team1 - 1\n  df_joint$Goals_Team2 = df_joint$Goals_Team2 - 1\n  \n  # Ensure all combinations from 0 to 5 are included\n  #all_combinations &lt;- expand.grid(Goals_Team1 = 0:5, Goals_Team2 = 0:5)\n  #df_joint &lt;- merge(all_combinations, df_joint, by = c(\"Goals_Team1\", \"Goals_Team2\"), all.x = TRUE)\n  #df_joint$Probability[is.na(df_joint$Probability)] &lt;- 0\n  \n  # Calculate outcomes\n  df_joint &lt;- df_joint %&gt;%\n    mutate(\n      Outcome = case_when(\n        Goals_Team1 &gt; Goals_Team2 ~ \"Win1\",\n        Goals_Team1 &lt; Goals_Team2 ~ \"Win2\",\n        TRUE ~ \"Draw\"\n      )\n    )\n  \n  # Calculate probabilities\n  prob_team1_win &lt;- sum(df_joint$Probability[df_joint$Outcome == \"Win1\"])\n  prob_team2_win &lt;- sum(df_joint$Probability[df_joint$Outcome == \"Win2\"])\n  prob_draw &lt;- sum(df_joint$Probability[df_joint$Outcome == \"Draw\"])\n  \n  # Print probabilities to the console\n  # cat(\"Probability of\", team1_name, \"winning: \", prob_team1_win, \"\\n\")\n  # cat(\"Probability of\", team2_name, \"winning: \", prob_team2_win, \"\\n\")\n  # cat(\"Probability of a draw: \", prob_draw, \"\\n\")\n  \n  # Plot the joint probabilities with labels and different colors for outcomes\n  d = df_joint %&gt;% filter(df_joint$Goals_Team1 &lt;= 5)  \n  joint_plot &lt;- ggplot(d, aes(x = Goals_Team1, y = Goals_Team2, fill = Outcome)) +\n    geom_tile(color = \"white\", aes(alpha = Probability)) +\n    geom_text(aes(label = sprintf(\"%.2f\", Probability)), color = \"black\", size = 3) +\n    scale_fill_manual(values = c(\"Win1\" = \"blue\", \"Win2\" = \"red\", \"Draw\" = \"green\"), guide = NULL) +\n    scale_alpha(range = c(0.3, 1), guide = NULL) +\n    labs(title = paste0(team1_name, \" vs. \", team2_name),  x = paste(\"Goals by\", team1_name), y = paste(\"Goals by\", team2_name)) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      axis.title = element_text(size = 12),\n      axis.text = element_text(size = 10)\n    ) +\n    scale_x_continuous(limits = c(-0.5, 9), breaks = 0:5) +\n    scale_y_continuous(limits = c(-0.5, 5.5), breaks = 0:5) +\n    annotate(\"text\", x = 5.7, y = 4, label = sprintf(\"%s Wins: %.2f\", team1_name, prob_team1_win), color = \"blue\", size = 3, hjust = 0) +\n    annotate(\"text\", x = 5.7, y = 3, label = sprintf(\"Draw: %.2f\", prob_draw), color = \"green\", size = 3, hjust = 0) +\n    annotate(\"text\", x = 5.7, y = 2, label = sprintf(\"%s Wins: %.2f\", team2_name, prob_team2_win), color = \"red\", size = 3, hjust = 0)\n  \n  # Print the joint plot\n  return(joint_plot)\n}\n\n# Call the function\nplot_goal_probabilities(attack_team1=attack_germany$A, defense_team1 = defense_germany$A, \n                        attack_team2=attack_scotland$A, defense_team2 = defense_scotland$A, team1_name=\"Germany\", team2_name=\"Scotland\")\n\n\n\n\n\n\n\n\n\nRemember the result? It was 5:1 for Germany, so quite unexpected by the model. So that these predictions with a grain of salt. The model is based on historical data and does not take into account the current form of the teams."
  },
  {
    "objectID": "Euro24/euro24.html#details-on-individual-games",
    "href": "Euro24/euro24.html#details-on-individual-games",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "Details on individual games",
    "text": "Details on individual games\n\nGoal Distributions"
  },
  {
    "objectID": "Euro24/euro24.html#germany-vs-scotland",
    "href": "Euro24/euro24.html#germany-vs-scotland",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "1 Germany vs Scotland",
    "text": "1 Germany vs Scotland"
  },
  {
    "objectID": "Euro24/euro24.html#hungary-vs-switzerland",
    "href": "Euro24/euro24.html#hungary-vs-switzerland",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "2 Hungary vs Switzerland",
    "text": "2 Hungary vs Switzerland"
  },
  {
    "objectID": "Euro24/euro24.html#spain-vs-croatia",
    "href": "Euro24/euro24.html#spain-vs-croatia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "3 Spain vs Croatia",
    "text": "3 Spain vs Croatia"
  },
  {
    "objectID": "Euro24/euro24.html#italy-vs-albania",
    "href": "Euro24/euro24.html#italy-vs-albania",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "4 Italy vs Albania",
    "text": "4 Italy vs Albania"
  },
  {
    "objectID": "Euro24/euro24.html#poland-vs-netherlands",
    "href": "Euro24/euro24.html#poland-vs-netherlands",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "5 Poland vs Netherlands",
    "text": "5 Poland vs Netherlands"
  },
  {
    "objectID": "Euro24/euro24.html#slovenia-vs-denmark",
    "href": "Euro24/euro24.html#slovenia-vs-denmark",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "6 Slovenia vs Denmark",
    "text": "6 Slovenia vs Denmark"
  },
  {
    "objectID": "Euro24/euro24.html#serbia-vs-england",
    "href": "Euro24/euro24.html#serbia-vs-england",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "7 Serbia vs England",
    "text": "7 Serbia vs England"
  },
  {
    "objectID": "Euro24/euro24.html#romania-vs-ukraine",
    "href": "Euro24/euro24.html#romania-vs-ukraine",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "8 Romania vs Ukraine",
    "text": "8 Romania vs Ukraine"
  },
  {
    "objectID": "Euro24/euro24.html#belgium-vs-slovakia",
    "href": "Euro24/euro24.html#belgium-vs-slovakia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "9 Belgium vs Slovakia",
    "text": "9 Belgium vs Slovakia"
  },
  {
    "objectID": "Euro24/euro24.html#austria-vs-france",
    "href": "Euro24/euro24.html#austria-vs-france",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "10 Austria vs France",
    "text": "10 Austria vs France"
  },
  {
    "objectID": "Euro24/euro24.html#t√ºrkiye-vs-georgia",
    "href": "Euro24/euro24.html#t√ºrkiye-vs-georgia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "11 T√ºrkiye vs Georgia",
    "text": "11 T√ºrkiye vs Georgia"
  },
  {
    "objectID": "Euro24/euro24.html#portugal-vs-czechia",
    "href": "Euro24/euro24.html#portugal-vs-czechia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "12 Portugal vs Czechia",
    "text": "12 Portugal vs Czechia"
  },
  {
    "objectID": "Euro24/euro24.html#croatia-vs-albania",
    "href": "Euro24/euro24.html#croatia-vs-albania",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "13 Croatia vs Albania",
    "text": "13 Croatia vs Albania"
  },
  {
    "objectID": "Euro24/euro24.html#germany-vs-hungary",
    "href": "Euro24/euro24.html#germany-vs-hungary",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "14 Germany vs Hungary",
    "text": "14 Germany vs Hungary"
  },
  {
    "objectID": "Euro24/euro24.html#scotland-vs-switzerland",
    "href": "Euro24/euro24.html#scotland-vs-switzerland",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "15 Scotland vs Switzerland",
    "text": "15 Scotland vs Switzerland"
  },
  {
    "objectID": "Euro24/euro24.html#spain-vs-italy",
    "href": "Euro24/euro24.html#spain-vs-italy",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "16 Spain vs Italy",
    "text": "16 Spain vs Italy"
  },
  {
    "objectID": "Euro24/euro24.html#slovenia-vs-serbia",
    "href": "Euro24/euro24.html#slovenia-vs-serbia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "17 Slovenia vs Serbia",
    "text": "17 Slovenia vs Serbia"
  },
  {
    "objectID": "Euro24/euro24.html#denmark-vs-england",
    "href": "Euro24/euro24.html#denmark-vs-england",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "18 Denmark vs England",
    "text": "18 Denmark vs England"
  },
  {
    "objectID": "Euro24/euro24.html#poland-vs-austria",
    "href": "Euro24/euro24.html#poland-vs-austria",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "19 Poland vs Austria",
    "text": "19 Poland vs Austria"
  },
  {
    "objectID": "Euro24/euro24.html#netherlands-vs-france",
    "href": "Euro24/euro24.html#netherlands-vs-france",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "20 Netherlands vs France",
    "text": "20 Netherlands vs France"
  },
  {
    "objectID": "Euro24/euro24.html#slovakia-vs-ukraine",
    "href": "Euro24/euro24.html#slovakia-vs-ukraine",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "21 Slovakia vs Ukraine",
    "text": "21 Slovakia vs Ukraine"
  },
  {
    "objectID": "Euro24/euro24.html#belgium-vs-romania",
    "href": "Euro24/euro24.html#belgium-vs-romania",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "22 Belgium vs Romania",
    "text": "22 Belgium vs Romania"
  },
  {
    "objectID": "Euro24/euro24.html#t√ºrkiye-vs-portugal",
    "href": "Euro24/euro24.html#t√ºrkiye-vs-portugal",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "23 T√ºrkiye vs Portugal",
    "text": "23 T√ºrkiye vs Portugal"
  },
  {
    "objectID": "Euro24/euro24.html#georgia-vs-czechia",
    "href": "Euro24/euro24.html#georgia-vs-czechia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "24 Georgia vs Czechia",
    "text": "24 Georgia vs Czechia"
  },
  {
    "objectID": "Euro24/euro24.html#switzerland-vs-germany",
    "href": "Euro24/euro24.html#switzerland-vs-germany",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "25 Switzerland vs Germany",
    "text": "25 Switzerland vs Germany"
  },
  {
    "objectID": "Euro24/euro24.html#scotland-vs-hungary",
    "href": "Euro24/euro24.html#scotland-vs-hungary",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "26 Scotland vs Hungary",
    "text": "26 Scotland vs Hungary"
  },
  {
    "objectID": "Euro24/euro24.html#croatia-vs-italy",
    "href": "Euro24/euro24.html#croatia-vs-italy",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "27 Croatia vs Italy",
    "text": "27 Croatia vs Italy"
  },
  {
    "objectID": "Euro24/euro24.html#albania-vs-spain",
    "href": "Euro24/euro24.html#albania-vs-spain",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "28 Albania vs Spain",
    "text": "28 Albania vs Spain"
  },
  {
    "objectID": "Euro24/euro24.html#netherlands-vs-austria",
    "href": "Euro24/euro24.html#netherlands-vs-austria",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "29 Netherlands vs Austria",
    "text": "29 Netherlands vs Austria"
  },
  {
    "objectID": "Euro24/euro24.html#france-vs-poland",
    "href": "Euro24/euro24.html#france-vs-poland",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "30 France vs Poland",
    "text": "30 France vs Poland"
  },
  {
    "objectID": "Euro24/euro24.html#england-vs-slovenia",
    "href": "Euro24/euro24.html#england-vs-slovenia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "31 England vs Slovenia",
    "text": "31 England vs Slovenia"
  },
  {
    "objectID": "Euro24/euro24.html#denmark-vs-serbia",
    "href": "Euro24/euro24.html#denmark-vs-serbia",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "32 Denmark vs Serbia",
    "text": "32 Denmark vs Serbia"
  },
  {
    "objectID": "Euro24/euro24.html#slovakia-vs-romania",
    "href": "Euro24/euro24.html#slovakia-vs-romania",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "33 Slovakia vs Romania",
    "text": "33 Slovakia vs Romania"
  },
  {
    "objectID": "Euro24/euro24.html#ukraine-vs-belgium",
    "href": "Euro24/euro24.html#ukraine-vs-belgium",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "34 Ukraine vs Belgium",
    "text": "34 Ukraine vs Belgium"
  },
  {
    "objectID": "Euro24/euro24.html#czechia-vs-t√ºrkiye",
    "href": "Euro24/euro24.html#czechia-vs-t√ºrkiye",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "35 Czechia vs T√ºrkiye",
    "text": "35 Czechia vs T√ºrkiye"
  },
  {
    "objectID": "Euro24/euro24.html#georgia-vs-portugal",
    "href": "Euro24/euro24.html#georgia-vs-portugal",
    "title": "Bayes is coming home - Predicting the Euro 2024 with Stan",
    "section": "36 Georgia vs Portugal",
    "text": "36 Georgia vs Portugal"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html",
    "href": "Euro24/comp_bundes_liga.html",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "",
    "text": "The experiments take some time to run, therefore we used the R-Script to producte the results https://github.com/oduerr/da/blob/master/website/Euro24/eval_performance_runner.R."
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#loading-the-data",
    "href": "Euro24/comp_bundes_liga.html#loading-the-data",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\n  df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23.csv')\n  df &lt;- mutate(df, name = sub(\"^.*/\", \"\", name))\n  \n  #Add additional run with home advantage\n  df2 = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_home_adv.csv')\n  df2 &lt;- mutate(df2, name = sub(\"^.*/\", \"\", name))\n  df &lt;- rbind(df, df2)\n  \n  # Add additional run with 18 games ahead\n  df3 = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_18Ahead.csv')\n  df = df3\n  print(\"Using 24_18Ahead that means all data is averaged 18 games ahead.\")\n\n\n[1] \"Using 24_18Ahead that means all data is averaged 18 games ahead.\"\n\n\nCode\n  #df = read.csv('~/Documents/GitHub/da/website/Euro24/eval_performance_bundesliga_23_bets05_18Ahead.csv')\n  \n  df_raw = read.csv('~/Documents/GitHub/da/website/Euro24/bundesliga2023.csv')\n  head(df_raw) %&gt;% kable()\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\nHS\nAS\nHST\nAST\nHF\nAF\nHC\nAC\nHY\nAY\nHR\nAR\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nVCH\nVCD\nVCA\nMaxH\nMaxD\nMaxA\nAvgH\nAvgD\nAvgA\nB365.2.5\nB365.2.5.1\nP.2.5\nP.2.5.1\nMax.2.5\nMax.2.5.1\nAvg.2.5\nAvg.2.5.1\nAHh\nB365AHH\nB365AHA\nPAHH\nPAHA\nMaxAHH\nMaxAHA\nAvgAHH\nAvgAHA\nB365CH\nB365CD\nB365CA\nBWCH\nBWCD\nBWCA\nIWCH\nIWCD\nIWCA\nPSCH\nPSCD\nPSCA\nWHCH\nWHCD\nWHCA\nVCCH\nVCCD\nVCCA\nMaxCH\nMaxCD\nMaxCA\nAvgCH\nAvgCD\nAvgCA\nB365C.2.5\nB365C.2.5.1\nPC.2.5\nPC.2.5.1\nMaxC.2.5\nMaxC.2.5.1\nAvgC.2.5\nAvgC.2.5.1\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\nD1\n18/08/2023\n19:30\nWerder Bremen\nBayern Munich\n0\n4\nA\n0\n1\nA\n6\n25\n1\n10\n16\n11\n0\n6\n2\n1\n0\n0\n7.50\n6.50\n1.30\n8.00\n6.0\n1.31\n7.00\n5.25\n1.40\n8.59\n6.36\n1.33\n8.00\n5.50\n1.22\n9.00\n6.0\n1.30\n9.50\n6.50\n1.40\n8.50\n6.09\n1.32\n1.33\n3.4\n1.35\n3.41\n1.37\n3.55\n1.33\n3.34\n1.75\n1.87\n2.06\n1.84\n2.07\nNA\nNA\n1.84\n2.01\n8.50\n6.0\n1.30\n8.50\n6.0\n1.3\n7.00\n5.25\n1.40\n8.80\n6.30\n1.31\n8.00\n5.50\n1.22\n9.50\n6.00\n1.29\n9.50\n6.50\n1.40\n8.69\n6.05\n1.31\n1.36\n3.20\n1.34\n3.38\n1.40\n3.58\n1.34\n3.29\n1.75\n1.85\n2.08\n1.88\n2.03\n1.92\n2.09\n1.85\n2.00\n\n\nD1\n19/08/2023\n14:30\nAugsburg\nM'gladbach\n4\n4\nD\n3\n3\nD\n20\n9\n8\n6\n9\n14\n8\n7\n1\n2\n0\n0\n2.70\n3.60\n2.45\n2.70\n3.5\n2.45\n2.75\n3.60\n2.45\n2.74\n3.78\n2.51\n2.50\n3.40\n2.30\n2.70\n3.7\n2.45\n2.80\n3.88\n2.54\n2.72\n3.69\n2.47\n1.62\n2.3\n1.63\n2.39\n1.66\n2.43\n1.61\n2.34\n0.00\n2.03\n1.87\n2.04\n1.87\n2.07\n1.90\n2.02\n1.84\n2.80\n3.6\n2.38\n2.75\n3.6\n2.4\n2.75\n3.65\n2.45\n2.92\n3.72\n2.43\n2.50\n3.40\n2.30\n2.75\n3.75\n2.38\n2.92\n3.87\n2.51\n2.79\n3.67\n2.42\n1.62\n2.30\n1.67\n2.33\n1.67\n2.40\n1.62\n2.31\n0.25\n1.82\n2.11\n1.83\n2.11\n1.83\n2.18\n1.77\n2.09\n\n\nD1\n19/08/2023\n14:30\nHoffenheim\nFreiburg\n1\n2\nA\n0\n2\nA\n24\n17\n5\n8\n9\n9\n4\n2\n0\n1\n0\n0\n2.30\n3.60\n2.90\n2.25\n3.5\n3.10\n2.30\n3.55\n2.95\n2.43\n3.67\n2.92\n2.10\n3.40\n2.80\n2.38\n3.6\n2.88\n2.50\n3.85\n3.10\n2.35\n3.63\n2.94\n1.67\n2.2\n1.67\n2.30\n1.71\n2.32\n1.66\n2.24\n-0.25\n2.06\n1.84\n2.11\n1.82\n2.11\n1.88\n2.03\n1.82\n2.15\n3.6\n3.10\n2.20\n3.6\n3.1\n2.25\n3.60\n3.00\n2.20\n3.74\n3.32\n2.10\n3.40\n2.80\n2.15\n3.75\n3.13\n2.30\n3.87\n3.32\n2.20\n3.68\n3.15\n1.67\n2.20\n1.69\n2.28\n1.70\n2.36\n1.65\n2.26\n-0.25\n1.91\n2.02\n1.92\n2.01\n1.98\n2.02\n1.91\n1.95\n\n\nD1\n19/08/2023\n14:30\nLeverkusen\nRB Leipzig\n3\n2\nH\n2\n1\nH\n11\n13\n7\n6\n13\n10\n4\n5\n1\n2\n0\n0\n2.45\n3.60\n2.75\n2.45\n3.6\n2.70\n2.50\n3.55\n2.70\n2.50\n3.61\n2.85\n2.30\n3.25\n2.60\n2.45\n3.6\n2.75\n2.56\n3.75\n2.91\n2.47\n3.62\n2.78\n1.67\n2.2\n1.70\n2.24\n1.77\n2.28\n1.68\n2.20\n0.00\n1.83\n2.07\n1.83\n2.09\n1.89\n2.11\n1.82\n2.05\n2.38\n3.6\n2.80\n2.37\n3.5\n2.8\n2.50\n3.60\n2.70\n2.48\n3.63\n2.98\n2.30\n3.25\n2.60\n2.40\n3.60\n2.80\n2.50\n3.78\n2.99\n2.44\n3.60\n2.81\n1.67\n2.20\n1.74\n2.20\n1.75\n2.32\n1.68\n2.19\n-0.25\n2.11\n1.82\n2.14\n1.84\n2.14\n1.84\n2.09\n1.78\n\n\nD1\n19/08/2023\n14:30\nStuttgart\nBochum\n5\n0\nH\n2\n0\nH\n19\n4\n9\n1\n5\n12\n7\n0\n1\n1\n0\n0\n1.65\n4.33\n4.50\n1.68\n4.0\n4.75\n1.70\n4.10\n4.50\n1.70\n4.25\n4.78\n1.57\n3.90\n4.33\n1.70\n4.1\n4.60\n1.77\n4.40\n5.00\n1.69\n4.17\n4.69\n1.62\n2.3\n1.66\n2.32\n1.71\n2.38\n1.64\n2.29\n-0.75\n1.88\n2.02\n1.89\n2.01\n1.95\n2.04\n1.86\n1.98\n1.73\n4.2\n4.20\n1.77\n3.9\n4.2\n1.77\n4.10\n4.20\n1.78\n4.16\n4.52\n1.67\n3.60\n4.00\n1.73\n4.10\n4.33\n1.85\n4.20\n4.52\n1.76\n4.02\n4.32\n1.62\n2.30\n1.67\n2.33\n1.70\n2.35\n1.64\n2.26\n-0.75\n1.98\n1.95\n1.99\n1.93\n2.06\n1.96\n1.95\n1.89\n\n\nD1\n19/08/2023\n14:30\nWolfsburg\nHeidenheim\n2\n0\nH\n2\n0\nH\n18\n13\n8\n1\n10\n9\n1\n7\n3\n0\n0\n0\n1.60\n4.20\n5.25\n1.61\n4.2\n5.25\n1.63\n4.20\n5.25\n1.62\n4.49\n5.24\n1.50\n4.00\n4.75\n1.60\n4.4\n5.00\n1.66\n4.60\n5.50\n1.62\n4.31\n5.19\n1.62\n2.3\n1.63\n2.39\n1.66\n2.44\n1.61\n2.34\n-1.00\n2.03\n1.87\n2.03\n1.88\n2.07\n1.88\n2.01\n1.84\n1.62\n4.2\n5.00\n1.65\n4.1\n4.8\n1.65\n4.20\n4.90\n1.66\n4.42\n5.16\n1.50\n4.00\n4.75\n1.62\n4.33\n5.00\n1.69\n4.50\n5.25\n1.64\n4.28\n5.03\n1.57\n2.38\n1.60\n2.49\n1.67\n2.49\n1.59\n2.39\n-1.00\n2.13\n1.81\n2.10\n1.82\n2.14\n1.85\n2.07\n1.80"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#exploratory-analysis",
    "href": "Euro24/comp_bundes_liga.html#exploratory-analysis",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\n\nCode\n#str(df_raw)\n#Full Time Home Goals, Full Time Away Goals\nhome_wins = df_raw$FTHG &gt; df_raw$FTAG\naway_wins = df_raw$FTHG &lt; df_raw$FTAG\ndraws = df_raw$FTHG == df_raw$FTAG\n\n# Betting on home wins would have some log(0) --&gt; -Inf\n\n# Betting according frequency\n# This is a bit cheating, since we use the future data to calculate\nps_naive = c(sum(home_wins), sum(draws), sum(away_wins))/nrow(df_raw)\nNLL_NAIVE = sum(\n  -home_wins*log(ps_naive[1])\n  -draws*log(ps_naive[2])\n  -away_wins*log(ps_naive[3])\n)/nrow(df_raw)\nNLL_NAIVE\n\n\n[1] 1.074078"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#exploratory-analysis-1",
    "href": "Euro24/comp_bundes_liga.html#exploratory-analysis-1",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\n\nCode\n#str(df_raw)\n\n# Load necessary libraries\nlibrary(dplyr)\n\n# Load your data\n# df_raw &lt;- read.csv('path_to_your_data.csv') # Replace with the actual data loading method\n\n# Example data frame structure for reference\n# df_raw &lt;- data.frame(\n#   B365H = c(7.5, 2.7, 2.3), B365D = c(6.5, 3.6, 3.6), B365A = c(1.3, 2.45, 2.9),\n#   FTR = c(\"A\", \"D\", \"A\")\n# )\n\n# Calculate implied probabilities\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    ImpliedProb_H = 1 / B365H,\n    ImpliedProb_D = 1 / B365D,\n    ImpliedProb_A = 1 / B365A\n  ) %&gt;%\n  mutate(\n    SumImpliedProbs = ImpliedProb_H + ImpliedProb_D + ImpliedProb_A,\n    ImpliedProb_H = ImpliedProb_H / SumImpliedProbs,\n    ImpliedProb_D = ImpliedProb_D / SumImpliedProbs,\n    ImpliedProb_A = ImpliedProb_A / SumImpliedProbs\n  )\n\n# Convert actual outcomes to binary indicators\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    Outcome_H = ifelse(FTR == \"H\", 1, 0),\n    Outcome_D = ifelse(FTR == \"D\", 1, 0),\n    Outcome_A = ifelse(FTR == \"A\", 1, 0)\n  )\n\n# Calculate negative log-likelihood for the given strategy\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    NLL_H = Outcome_H * log(ImpliedProb_H),\n    NLL_D = Outcome_D * log(ImpliedProb_D),\n    NLL_A = Outcome_A * log(ImpliedProb_A),\n    NLL = -(NLL_H + NLL_D + NLL_A)\n  )\n\n# Mean negative log-likelihood for the given strategy\nmean_nll &lt;- mean(df_raw$NLL, na.rm = TRUE)\n\ncat(\"Mean Negative Log-Likelihood for given strategy:\", mean_nll, \"\\n\")\n\n\nMean Negative Log-Likelihood for given strategy: 0.9463587 \n\n\nCode\n# Calculate negative log-likelihood for uniform betting (1/3 for each outcome)\nbaseline_prob &lt;- 1 / 3\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    BaselineNLL_H = Outcome_H * log(baseline_prob),\n    BaselineNLL_D = Outcome_D * log(baseline_prob),\n    BaselineNLL_A = Outcome_A * log(baseline_prob),\n    BaselineNLL = -(BaselineNLL_H + BaselineNLL_D + BaselineNLL_A)\n  )\n\n# Mean negative log-likelihood for uniform betting\nmean_baseline_nll &lt;- mean(df_raw$BaselineNLL, na.rm = TRUE)\n\ncat(\"Mean Negative Log-Likelihood for uniform betting:\", mean_baseline_nll, \"\\n\")\n\n\nMean Negative Log-Likelihood for uniform betting: 1.098612 \n\n\nCode\n# Calculate negative log-likelihood for always betting on home win\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    AlwaysHomeNLL_H = -log(baseline_prob) * Outcome_H,\n    AlwaysHomeNLL_L = -log(1 - baseline_prob) * (1 - Outcome_H),\n    AlwaysHomeNLL = AlwaysHomeNLL_H + AlwaysHomeNLL_L\n  )\nmean_always_home_nll &lt;- mean(df_raw$AlwaysHomeNLL, na.rm = TRUE)\n\ncat(\"Mean Negative Log-Likelihood for always betting on home win:\", mean_always_home_nll, \"\\n\")\n\n\nMean Negative Log-Likelihood for always betting on home win: 0.7090001 \n\n\nCode\n# Calculate negative log-likelihood for always betting on away win\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    AlwaysAwayNLL_A = -log(baseline_prob) * Outcome_A,\n    AlwaysAwayNLL_L = -log(1 - baseline_prob) * (1 - Outcome_A),\n    AlwaysAwayNLL = AlwaysAwayNLL_A + AlwaysAwayNLL_L\n  )\nmean_always_away_nll &lt;- mean(df_raw$AlwaysAwayNLL, na.rm = TRUE)\n\ncat(\"Mean Negative Log-Likelihood for always betting on away win:\", mean_always_away_nll, \"\\n\")\n\n\nMean Negative Log-Likelihood for always betting on away win: 0.6115971 \n\n\nCode\n# Calculate negative log-likelihood for always betting on draw\ndf_raw &lt;- df_raw %&gt;%\n  mutate(\n    AlwaysDrawNLL_D = -log(baseline_prob) * Outcome_D,\n    AlwaysDrawNLL_L = -log(1 - baseline_prob) * (1 - Outcome_D),\n    AlwaysDrawNLL = AlwaysDrawNLL_D + AlwaysDrawNLL_L\n  )\nmean_always_draw_nll &lt;- mean(df_raw$AlwaysDrawNLL, na.rm = TRUE)\n\ncat(\"Mean Negative Log-Likelihood for always betting on draw:\", mean_always_draw_nll, \"\\n\")\n\n\nMean Negative Log-Likelihood for always betting on draw: 0.5889452"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#model-comparisons",
    "href": "Euro24/comp_bundes_liga.html#model-comparisons",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Model Comparisons",
    "text": "Model Comparisons\nWe use the negative log likelihood (NLL) as a measure of the predictive performance of the models. The lower the NLL, the better the model. However, strictly speaking it is the negative log posterior predictive density (divided by \\(n\\)) evaluated at the \\(n\\) games after the training data.\n\\[\n\\text{NLL} = -\\frac{1}{n}\\sum_{i=1}^n \\log p(y_i | x_i, \\theta)\n\\]\n\n\nCode\n   # Assuming df is your dataframe\n  df %&gt;% filter(type == 'NLL_PRED') %&gt;% \n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line() + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset', \n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    #ylim(2.9, 3.5) +\n    #xlim(0,100) +\n    theme(legend.position = \"top\") +\n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations\n\nEspecially for small training data, the hierarchical model performs better than the non-hierarchical model.\nThe Correlated Dataset model performs slightly better than non-correlated one\nThere is partically no difference in predictive performance when comparing the model with and without Cholesky decomposition.\nThe negative binomial model performs comparable to Poisson model."
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#comparison-of-predicted-vs-psis-loo",
    "href": "Euro24/comp_bundes_liga.html#comparison-of-predicted-vs-psis-loo",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Comparison of predicted vs PSIS-LOO",
    "text": "Comparison of predicted vs PSIS-LOO\n\n\nCode\n  df %&gt;% filter(type %in% c('NLL_PRED', 'NLL_PSIS', 'NLL_PRED_STAN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = type)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title =  'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(2.5, 4) +\n    facet_wrap(~name) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\nObservations"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#nnl-for-win-draws-and-losses",
    "href": "Euro24/comp_bundes_liga.html#nnl-for-win-draws-and-losses",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "NNL for win, draws and losses",
    "text": "NNL for win, draws and losses\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Negative Log Likelihood'\n    ) + \n    ylim(0.75, 1.5) +\n    geom_hline(yintercept = NLL_NAIVE, col = \"green\", alpha=0.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\nCode\n#### Booki mean\n  dfp = df %&gt;% \n    filter(type %in% c('NLL_RESULTS', 'NLL_BOOKIE')) %&gt;%\n    group_by(name, type) %&gt;% \n    summarise(nll = mean(res)) \n  \n  dfp = rbind(dfp, data.frame(name = \"NLL_NAIVE\", type = \"NLL_NAIVE\", nll = NLL_NAIVE))\n  # Remove all but one NLL_BOOKIE\n  kableExtra::kable(dfp, digits = 3)\n\n\n\n\n\n\nname\ntype\nnll\n\n\n\n\nda/stan/football/hier_model_nb\nNLL_BOOKIE\n0.953\n\n\nda/stan/football/hier_model_nb\nNLL_RESULTS\n0.983\n\n\nda/stan/football/non_hier_model\nNLL_BOOKIE\n0.953\n\n\nda/stan/football/non_hier_model\nNLL_RESULTS\n1.011\n\n\nda/website/Euro24/hier_model\nNLL_BOOKIE\n0.953\n\n\nda/website/Euro24/hier_model\nNLL_RESULTS\n0.983\n\n\nda/website/Euro24/hier_model_cor\nNLL_BOOKIE\n0.953\n\n\nda/website/Euro24/hier_model_cor\nNLL_RESULTS\n0.992\n\n\nda/website/Euro24/hier_model_cor_home\nNLL_BOOKIE\n0.953\n\n\nda/website/Euro24/hier_model_cor_home\nNLL_RESULTS\n0.992\n\n\nda/website/Euro24/hier_model_cor_nocholsky\nNLL_BOOKIE\n0.953\n\n\nda/website/Euro24/hier_model_cor_nocholsky\nNLL_RESULTS\n0.992\n\n\nNLL_NAIVE\nNLL_NAIVE\n1.074\n\n\n\n\n\n\n\n\n\nObservations\n\nThe NLL for the bookie is always better than the NLL of the models, so we should not bet.\n\n\n\nRanked probability score\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('RPS', 'rps_booki')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = type)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Ranked Probability Score'\n    ) + \n    #ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\nAveraged for the complete season:\n\n\nCode\n  library(dplyr)\n  dfres = df %&gt;% \n    filter(type %in% c('RPS', 'rps_booki')) %&gt;%\n    group_by(name, type) %&gt;% \n    summarise(rps = mean(res)) \n\n\n`summarise()` has grouped output by 'name'. You can override using the\n`.groups` argument.\n\n\nCode\n  rps_booki = dfres %&gt;% filter(type == 'rps_booki') \n  \n  dfres2 = dfres %&gt;% filter(type == 'RPS') %&gt;% dplyr::select(name, rps) \n  dfres2 = rbind(dfres2, rps_booki[1,])\n\n  # Sort by RPS\n  dfres2 = dfres2[order(dfres2$rps),]\n  kable(dfres2)\n\n\n\n\n\n\nname\nrps\ntype\n\n\n\n\nda/stan/football/hier_model_nb\n0.1865408\nrps_booki\n\n\nda/website/Euro24/hier_model_cor_home\n0.1979763\nNA\n\n\nda/stan/football/hier_model_nb\n0.1982434\nNA\n\n\nda/website/Euro24/hier_model\n0.1984227\nNA\n\n\nda/website/Euro24/hier_model_cor\n0.1993777\nNA\n\n\nda/website/Euro24/hier_model_cor_nocholsky\n0.1994641\nNA\n\n\nda/stan/football/non_hier_model\n0.2032188\nNA"
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#betting-returns",
    "href": "Euro24/comp_bundes_liga.html#betting-returns",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Betting Returns",
    "text": "Betting Returns\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('BET_RETURN')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) +  \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Betting Returns'\n    ) + \n    #ylim(0.75, 1.5) +\n    theme(legend.position = \"bottom\") +  \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('BET_RETURN')) %&gt;%\n    group_by(name) %&gt;% \n    summarise(mean(res))\n\n\n# A tibble: 6 √ó 2\n  name                                       `mean(res)`\n  &lt;chr&gt;                                            &lt;dbl&gt;\n1 da/stan/football/hier_model_nb                  0.0635\n2 da/stan/football/non_hier_model                 0.0137\n3 da/website/Euro24/hier_model                    0.0660\n4 da/website/Euro24/hier_model_cor               -0.120 \n5 da/website/Euro24/hier_model_cor_home          -0.0905\n6 da/website/Euro24/hier_model_cor_nocholsky     -0.122 \n\n\n\nObservations\nWe see quite some fluctuation in the betting return. Since the NLL shows that the odds from the booki are always better then the NLLs of the models we should not bet."
  },
  {
    "objectID": "Euro24/comp_bundes_liga.html#technical-details",
    "href": "Euro24/comp_bundes_liga.html#technical-details",
    "title": "Comparison of different models Bundesliga Dataset",
    "section": "Technical Details",
    "text": "Technical Details\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('MIN_SUM_PROB')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Sum of Probabilities from 0 to 10 goals (should be 1)'\n    ) + \n    ylim(0.75, 1.01) +\n    theme(legend.position = \"bottom\") + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area\n\n\n\n\n\n\n\n\n\n\n\nCode\n  df %&gt;% \n    filter(type %in% c('num_divergent')) %&gt;%\n    ggplot(aes(x = ntrain, y = res, color = name)) + \n    geom_line(aes(linetype = name)) + \n    geom_point() + \n    theme_minimal() + \n    labs(\n      title = 'Comparison of different models for the Bundesliga 2023 dataset',\n      x = 'Number of training data', \n      y = 'Number of Divergent Transitions (sqrt scale)'\n    ) + \n    theme(legend.position = \"bottom\") + \n    scale_y_sqrt() + \n    coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area \n\n\n\n\n\n\n\n\n\n\n\nCode\ndf %&gt;% \n  filter(type %in% c('ebfmi')) %&gt;%\n  ggplot(aes(x = ntrain, y = res, color = name)) + \n  geom_line(aes(linetype = name)) + \n  geom_point() + \n  theme_minimal() + \n  labs(\n    title = 'Comparison of different models for the Bundesliga 2023 dataset',\n    x = 'Number of training data', \n    y = 'ebfmi'\n  ) + \n  theme(legend.position = \"bottom\") + \n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = Inf, y = 0.33, label = \"Acceptable\", hjust = 1.1, color = \"red\") +\n  annotate(\"text\", x = Inf, y = 0.27, label = \"Non-Acceptable\", hjust = 1.1, color = \"red\") +\n  coord_cartesian(clip = \"off\") # Allow lines to go outside the plot area"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Internal Notes for authoring the website\n\nHow to add a new page\n\nCreate a new quarto file in the root directory or subdirectory. Put the quarto-file in website directory (or subdirectory). Make sure that you do not have self-contained: true in the qmd file, otherwise file gets to large. An example is website/lecture_notes/MCMC/MCMC.qmd\n\n\n---\ntitle: \"Lecture Notes on MCMC\"\nauthor: \"Oliver D√ºrr\"\nformat: \n  html:\n    toc: true\n    toc-title: \"Table of Contents\"\n    toc-depth: 3\n    fig-width: 6\n    fig-height: 3\n    code-fold: true\n    code-tools: true\n    mathjax: true\n  #  self-contained: true &lt;------ NEEDS TO BE COMMENTED OUT (or not there at all)\n  # pdf:\n  #   toc: true\n  #   toc-title: \"Table of Contents\"\nfilters:\n  - webr\n---\n\n\n\nHow to render the website\n\nRender and move to docs folder\n\nquarto render \n\nCheck in the changes in the docs folder\n\n\n\nDevelopment (in da/website)\nquarto preview lecture_notes/MCMC/MCMC.qmd #For a single file\nquarto preview #For the complete website"
  }
]