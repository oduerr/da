[
  {
    "objectID": "Stan_Primer_Full.html",
    "href": "Stan_Primer_Full.html",
    "title": "Stan primer",
    "section": "",
    "text": "MCMC with Stan / Simple diagnostics"
  },
  {
    "objectID": "Stan_Primer_Full.html#cmdstan",
    "href": "Stan_Primer_Full.html#cmdstan",
    "title": "Stan primer",
    "section": "Cmdstan",
    "text": "Cmdstan\n\nlibrary(cmdstanr)\nm_rcmdstan &lt;- cmdstan_model(file_stan) #Compiling\ns_rcmdstan = m_rcmdstan$sample(data = data) #Sampling\ndf = s_rcmdstan$draws(format = \"df\") #Extracting Samples as data.frame\n\nPrinting the model in markdown\n{r, echo=lsg, eval=FALSE, code=readLines(“../world.stan”), collapse=TRUE}"
  },
  {
    "objectID": "Stan_Primer_Full.html#tidy-bayes",
    "href": "Stan_Primer_Full.html#tidy-bayes",
    "title": "Stan primer",
    "section": "Tidy Bayes",
    "text": "Tidy Bayes\nWorks with rstan and cmdstanr\n\nlibrary(tidybayes)\nspread_draws(s, c(a,b)) #Extracts a and b from s (stan or cmdstan samples)\nspread_draws(s, f[i])  #Extracts vector f and calls components i"
  },
  {
    "objectID": "Stan_Primer_Full.html#bayes-plot",
    "href": "Stan_Primer_Full.html#bayes-plot",
    "title": "Stan primer",
    "section": "Bayes plot",
    "text": "Bayes plot\nWorks with rstan and cmdstanr\n\n  s = s_rcmdstan$draws() #We need to call draws() 2023\n  bayesplot::mcmc_trace(s)"
  },
  {
    "objectID": "Stan_Primer_Full.html#stan",
    "href": "Stan_Primer_Full.html#stan",
    "title": "Stan primer",
    "section": "Stan",
    "text": "Stan\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(javascript=FALSE) #Prevents freezes of RStudio (Jan 2023) \n\nm_rstan = stan_model(model_code = stan_code) #Compiling from string\nm_rstan = stan_model(file='mymodel.stan') #Compiling from file\ns_rstan = sampling(m_rstan, data=data) #Sampling from file\n\n#No MCMC just evaluating parameters\nsampling(model_1, data=dat, algorithm=\"Fixed_param\", chain=1, iter=1)"
  },
  {
    "objectID": "Stan_Primer_Full.html#other-cheat-sheets",
    "href": "Stan_Primer_Full.html#other-cheat-sheets",
    "title": "Stan primer",
    "section": "Other Cheat Sheets",
    "text": "Other Cheat Sheets\n\nhttp://www.sumsar.net/files/posts/2017-bayesian-tutorial-exercises/stan_cheat_sheet2.12.pdf\nhttps://github.com/sieste/Stan_cheatsheet"
  },
  {
    "objectID": "Stan_Primer_Full.html#data-used",
    "href": "Stan_Primer_Full.html#data-used",
    "title": "Stan primer",
    "section": "Data used",
    "text": "Data used\nSome Data for linear regression\n\nN = 4\nx = c(-2.,-0.66666, 0.666, 2.)\ny = c(-6.25027354, -2.50213382, -6.07525495,  7.92081243)\ndata = list(N=N, x=x, y=y)"
  },
  {
    "objectID": "Stan_Primer_Full.html#getting-samples-from-the-posterior",
    "href": "Stan_Primer_Full.html#getting-samples-from-the-posterior",
    "title": "Stan primer",
    "section": "Getting samples from the posterior",
    "text": "Getting samples from the posterior\nThere are currently (2023) two interfaces to stan from R. RStan (https://mc-stan.org/users/interfaces/rstan) which is a bit slower and does not use the latest stan compiler and rcmdstan (https://mc-stan.org/cmdstanr/articles/cmdstanr.html).\n\nThe technical steps\nThere are 3 steps:\n\nDefining the model\nCompiling the model. In this step C code is generated.\nRunning the simulation / sampling from the posterior\nExtracting the samples from the posterior\n\n\n\nDefinition\nTo define a model, you can add a string or create a .stan file. Another option is to use a Stan markdown chunk and output.var=my_model to the name of the model. Code completion and highlighting is working for files and code chunks.\n\nstan_code = \"data{\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n}\n\nparameters{\n  real a; //Instead of using e.g. half Gaussian\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel{\n  //y ~ normal(mu, sigma);\n  y ~ normal(a * x + b, sigma);\n  a ~ normal(3, 10); \n  b ~ normal(0, 10); \n  sigma ~ normal(0,10);\n}\"\n\n\n\nCompiling\nCompiling works with:\n\nCompiling rstan stan_model(model_code = stan_code) or stan_model(file='mymodel.stan')\nCompiling cmdstan cmdstan_model(file_stan)\n\n\nCompiling with rstan\n\n  library(rstan)\n  m_rstan = stan_model(model_code = stan_code)\n\n\n\nCompiling with rcmdstan\nThere is no possibility to use a string for cmd_stan.\n\n  m_rcmdstan &lt;- cmdstan_model(stan_file='stan/simple_lr.stan')\n  m_rcmdstan$print() #Displays the used model\n\n\n\n\nSampling / running the chains\nFor rstan: sampling(m_rstan, data=data) For cmdstan: mod$sample(data = data_file, seed=123)\n\n  s_rcmdstan = m_rcmdstan$sample(data = data)\n\n\n  s_rstan = sampling(m_rstan)  \n\n\n\nDiagnostics of the chains\nThe package bayesplot can handle both interfaces\n\nTrace\n\n#traceplot(s_rstan, 'a')\n#bayesplot::mcmc_trace(s_rstan)\nbayesplot::mcmc_trace(s_rcmdstan$draws()) #similar result\ns_rstan\n# Rhat close to one and n_eff lager than half the number of draws; look fine\n\nKey Numbers\n\nRhat is something like the ratio of variation between the chains to withing the chains\nn_eff number of effective samples taking the autocorrelation into account (in cmd_stan the output is ess_bulk and ess_tail)\n\n\n\nShiny Stan\n\n  #Shiny Stan, quite overwhelming\n  library(shinystan)\n  launch_shinystan(s_rcmdstan)\n  \n  #If not working\n  #See https://discourse.mc-stan.org/t/will-launch-shinystan-work-soon-for-cmdstanr/17269\n  stanfit = rstan::read_stan_csv(s_rcmdstan$output_files())\n  launch_shinystan(stanfit)\n\n\n\n\nPosteriors (of the parameters)\nGetting the samples can be done as\n\nRstan extract(s_rstan)\ncmdstan s_rcmdstan$draws()\n\nAnother handy package is tidybayes which can handle the output of rstan and rcmdstan\n\nTidybayes\n\nlibrary(tidybayes)\nhead(spread_draws(s_rcmdstan, c(a,b))) #Non-tidy a and b in one row\n\n# A tibble: 6 × 5\n  .chain .iteration .draw     a      b\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1      1          1     1 1.99  -1.76 \n2      1          2     2 3.04   2.89 \n3      1          3     3 4.95  -3.39 \n4      1          4     4 0.934 -2.30 \n5      1          5     5 2.36   0.303\n6      1          6     6 5.31  -2.45 \n\nhead(gather_draws(s_rcmdstan, c(a,b))) #The ggplot like syntax\n\n# A tibble: 6 × 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable .value\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1      1          1     1 a          1.99 \n2      1          2     2 a          3.04 \n3      1          3     3 a          4.95 \n4      1          4     4 a          0.934\n5      1          5     5 a          2.36 \n6      1          6     6 a          5.31 \n\n#spread_draws(model_weight_sex, a[sex]) for multilevel models\n\n\n\nCommand Stan Functions\nFor more see: https://mc-stan.org/cmdstanr/articles/cmdstanr.html\n\ndf &lt;- s_rcmdstan$draws(format = \"df\")\ndf %&gt;% select(-c(.chain, .iteration, .draw)) %&gt;% cor()\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n            lp__           a           b      sigma\nlp__   1.0000000  0.12991773 -0.10625738 -0.6907346\na      0.1299177  1.00000000 -0.03857131 -0.1040964\nb     -0.1062574 -0.03857131  1.00000000  0.1078610\nsigma -0.6907346 -0.10409641  0.10786104  1.0000000\n\n\n\n\nStan functions\n\n  #plot(samples)\n  #samples = extract(s_rstan)\n  stan_dens(s_rstan)\n\n\n\n\n\n\n\n  #Note that these are marginals!\n\n\n\n“Manually” visualize the posterior\nWe extract samples from the posterior via extract (in some installation the wrong extract function is taken in that case use rstan::extract to use the right one). Visualize the posterior distribution of \\(a\\) from the samples.\n\n# Extract samples\npost = rstan::extract(s_rstan)\nT = length(post$a)\nhist(post$a,100, freq=F)\nlines(density(post$a),col='red')  \n\n\n\n\n\n\n\n\n\n\nPairs plot\nUsing the pairs plot, correlations in the variables can be found.\n\nnp_cp = bayesplot::nuts_params(s_rstan)\nbayesplot::mcmc_pairs(s_rstan, np = np_cp,pars = c(\"a\",\"b\"))\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Plots\n\n\nTask: Use the samples to create the following posterior predictive plots\nSome background first: posterior predictive distribution: \\[\n  p(y|x, D) =  \\int p(y|x,\\theta) p(\\theta|D) \\; d\\theta\n\\] Instead of integration, we sample in two turns\n\n\\(\\theta_i \\sim p(\\theta|D)\\)\n\\(y_{ix} \\sim p(y|x,\\theta_i)\\) #We do this for many x in practice\n\n\nCreation of the posterior predictive samples by hand\nYou can either do this part, or use stan to create the posterior predictive samples \\(y_{ix}\\) from the samples \\(\\theta_i\\) by hand.\nTip: Create two matrices yix and muix from the posterior samples of \\(a,b,\\sigma\\) with dimension (rows = number of posterior samples and cols = number of x positions).\n\nxs = -10:15 # The x-range 17 values from -1 to 15\nM = length(xs) \nyix = matrix(nrow=T, ncol = M) #Matrix from samples (number of posterior draws vs number of xs)\nmuix = matrix(nrow=T, ncol = M) #Matrix from mu (number of posterior draws vs number of xs)\nfor (i in 1:T){ #Samples from the posterior\n  a = post$a[i] #Corresponds to samples from theta\n  b = post$b[i]\n  sigma = post$sigma[i]\n  for (j in 1:M){ #Different values of X\n    mu = a * xs[j] + b\n    muix[i,j] = a * xs[j] + b\n    yix[i,j] = rnorm(1, mu, sigma) # Single number drawn\n  }\n}\n\nif (FALSE){\n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='mu=a*x+b')\n  for (i in 1:100){\n    lines(xs, muix[i,],lwd=0.25,col='blue')\n  }\n  \n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='ys')\n  for (i in 1:100){\n    points(xs, yix[i,], pch='.',col='red')\n  }\n}\n\nAfter you created the matrices yix and muix you can use the following function to draw the lines for the quantiles.\n\nplot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='quantiles (y and mu)')\nquant_lines = function(x2, y_pred, col='blue'){\n  m = apply(y_pred, 2,quantile, probs=c(0.50))\n  lines(x2, m,col=col)\n  q05 = apply(y_pred, 2, quantile, probs=c(0.25))\n  q95 = apply(y_pred, 2, quantile, probs=c(0.75))\n  lines(x2, q05,col=col)\n  lines(x2, q95,col=col)  \n}\n\nquant_lines(xs,yix, col='red')\nquant_lines(xs,muix, col='blue')\n\n\n\n\n\n\n\n\n\n\nCreation of the posterior predictive samples with Stan\nIt’s also possible to draw posterior predictive samples. One can use the generated quantities code block for that.\ndata{\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n  //For the prediced distribution (new)\n  int&lt;lower=0&gt; N2;\n  vector[N2] x2;\n}\n\ngenerated quantities {\n  real Y_predict[N2]; \n  for (i in 1:N2){\n    Y_predict = normal_rng(a * x2 + b, sigma);\n  }\n}\n\n  x2 = -10:15\n  N2 = length(x2)\n  fit2 = rstan::stan(file = '~/Dropbox/__HTWG/DataAnalytics/_Current/lab/06_03_Bayes_3/Stan_Primer_model_pred.stan',\n                         data=list(N=N,x=x, y=y, N2=N2,x2=x2),iter=10000)\n\n\n  d2 = rstan::extract(fit2)\n  y_pred = d2$Y_predict\n  dim(y_pred)\n  plot(x, y, xlim=c(-10,15), ylim=c(-25,25), ylab='quantiles (y and mu)')\n  quant_lines(x2,y_pred, col='red')\n\n\n\n\nNotes on creating excercises\n\nInclusion of Stan code\nIn case of cmdrstan the Stan code should be in an extra file to make the workflow easier and reuse compiled models. In case of rstan the Stan code can be included in the Rmd file as:\n{r, echo=TRUE, eval=FALSE, code=readLines(“../world.stan”), collapse=TRUE, echo=TRUE}"
  },
  {
    "objectID": "Stan_Primer_Full.html#rstudio-bug-2023",
    "href": "Stan_Primer_Full.html#rstudio-bug-2023",
    "title": "Stan primer",
    "section": "RStudio bug 2023",
    "text": "RStudio bug 2023\nSometimes R gets slow when rendering with stan code. At least in my case\nrstan_options(javascript=FALSE)\nNot observed anymore (2024)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "da_website",
    "section": "",
    "text": "This is a collection of bits and pieces useful in connection with data analysis."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html",
    "href": "lecture_notes/MCMC/MCMC.html",
    "title": "Lecture Notes on MCMC",
    "section": "",
    "text": "In this lecture note, we will discuss the basics of Markov Chain Monte Carlo (MCMC) methods. MCMC methods are a class of algorithms that are used to sample from a probability distribution \\(p(\\theta)\\) such as the posterior \\(p(\\theta|D) \\propto p(D|\\theta) p(\\theta)\\). While there are more advanced MCMC methods, we will focus on the Metropolis-Hastings algorithm, which is a simple and widely used MCMC algorithm and shows the principles of MCMC methods."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#a-note-on-notation",
    "href": "lecture_notes/MCMC/MCMC.html#a-note-on-notation",
    "title": "Lecture Notes on MCMC",
    "section": "📝 A note on notation",
    "text": "📝 A note on notation\nIn this lecture note, we will use the following notation:\n\n\\(\\theta\\) is a parameter of interest. This could be a scalar or a vector.\n\\(D\\) is the data.\n\\(p(\\theta)\\) is the prior distribution of \\(\\theta\\) and\n\\(p(D|\\theta)\\) is the likelihood of the data given \\(\\theta\\).\n\\(p(\\theta|D)\\) is the posterior distribution of \\(\\theta\\) given the data \\(D\\)\n\nAll expressions are read right to left, e.g. \\(p(\\theta|D)\\) is the posterior distribution of \\(\\theta\\) given the data \\(D\\) and \\(P_{ij}\\) is the probability of moving from state \\(j\\) to state \\(i\\).\n\nUsing \\(p(\\theta)\\) instead of \\(p(\\theta|D)\\)\nSince the MCMC methods works with any probability distribution, we will use \\(p(\\theta)\\) to denote the target distribution that we want to sample from. In the context of Bayesian inference, \\(p(\\theta)\\) is the posterior distribution \\(p(\\theta|D)\\) without explicitly stating the data \\(D\\). You can also think of it as the posterior when you have no data (the prior).\n\n\n📊 Densities vs. 📉 probabilities\nWhile in general in the lecture we will use the terms “density” and “probability” interchangeably. However, in the context of MCMC methods, we will use the term “density” to refer to the probability density function of a continuous distribution and “probability” to refer to the probability mass function of a discrete distribution. In case the destinction is crucial, we will use the terms \\(P(\\cdot)\\) for probabilities and \\(p(\\cdot)\\) for densities."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#a-high-level-overview-of-markov-chains.",
    "href": "lecture_notes/MCMC/MCMC.html#a-high-level-overview-of-markov-chains.",
    "title": "Lecture Notes on MCMC",
    "section": "🔭 A high level overview of Markov Chains.",
    "text": "🔭 A high level overview of Markov Chains.\nA Markov chain is a sequence of random variables \\(\\theta_1, \\theta_2, \\theta_3, \\ldots\\) which can be viewed as a time series where the next value depends only on the current value (Markov Property). In Figure 1: we show two Markov chains, starting from different initial values. Note that after about 150 steps both chains fluctuate around the same value. This is the stationary distribution of the Markov chain. A Markov chain reaches a stationary distribution when the distribution of the states no longer changes as the chain progresses.\n\n\n\n\n\n\nFigure 1: Shown are two Markov chains for the scalar quantity $\\theta$. The chains start from different initial positions. After about 150 time steps the chains fluctate around the same value.\n\n\n\nIn Figure 2, we show a histogram1 of the values of the two chains after from 250 steps onward. The histogram indicate that the values of the chains are distributed around the same distribution is the stationary distribution of the Markov chain.\n\n\n\n\n\n\nFigure 2: The Histogram (actually density estimates) for of the two chains after 250 steps\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe key idea of MCMC methods is to construct a Markov chain that has a stationary distribution equal to the target distribution \\(p(\\theta)\\). Once the Markov chain has converged to the stationary distribution, we can use the samples from the chain to approximate the target distribution.\n\n\nSo how do we construct a Markov chain with a stationary distribution equal to the target distribution \\(p(\\theta)\\)? To do so, we draw insights from statistical physics.\n\nStatistical physics and detailed balance\nConsider a glass of water in which we add a drop of ink. The ink will spread out in the water until it is uniformly distributed. This is an example of a system that has reached equilibrium. The the following ideas, are important inspirations for MCMC methods:\nErgodic Hypothesis - The diffusion process can be viewed as either a single particle or many particles. Initially concentrated in one region, a particle will eventually explore the entire space uniformly, regardless of its starting point. This idea, known as the Ergodic Hypothesis, was first proposed by Boltzmann.\nThe reaching of the stationary distribution - Over time, particles reach a stationary distribution (in physics is referred to as the equilibrium distribution). For example, ink in water eventually reaches a uniform distribution. However, not all distributions are uniform. For instance, the barometric height equation describes how particles in the air are more concentrated at the ground and decrease exponentially with height.\nDetailed Balance - At stationarity, the rate at which particles move from one point to another is equal to the rate at which they move in the opposite direction. This concept is known as detailed balance and is crucial for understanding the Metropolis-Hastings algorithm.\nFrom that, we can design the MCMC Algorithms algorithms. Roughly as follows * Start with parameter of interest \\(\\theta\\) at certain position * Mimics the diffusion process but with a stationary distribution of our choosing \\(p(\\theta)\\) * Algorithm needs to obey the detailed balance condition, this allow us to define the transition probabiliy with which we go from one value to another."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#detailed-balance",
    "href": "lecture_notes/MCMC/MCMC.html#detailed-balance",
    "title": "Lecture Notes on MCMC",
    "section": "⚖️ Detailed balance",
    "text": "⚖️ Detailed balance\nFor a Markov chain to be useful, it needs to convert to a stationary distribution and the stationary distribution needs to be the target distribution \\(p(\\theta)\\). The Metropolis-Hastings algorithm is a simple MCMC algorithm that ensures that the Markov chain converges to the desired stationary distribution. The Metropolis-Hastings algorithm is based on the principle of detailed balance. Also almost all other MCMC algorithms (such as Hamiltonian Monte Carlo, Slice Sampling, and Gibbs Sampling) are based on the principle of detailed balance. So what is detailed balance?\nFor the ink particles, we have a uniforn distribution as stationary distribution. For the ink particles in the water glass the detailed balance condition is that the rate at which particles move from one point to another is equal to the rate at which they move in the opposite direction. Remember the example with the king visiting the islands? The king will visit the islands with a probability proportional to the size on the island. So the stationary distribution is not uniform anymore. The detailed balance condition can be used to determine the probability \\(P_{ji}\\) of moving from island \\(i\\) to island \\(j\\). Now consider N Kings, in order that the number of Kings on the islands have reached a stationary distribution, at each time step as many Kings need to move from one island to another as move in the opposite direction. Otherwise the number of Kings on the islands would change over time and we have not yet reached the stationary distribution. The detailed balance condition is (a bit slopy) given by:\n\\[\n  P_{ij} N_j = P_{ji} N_i\n\\]\nwith \\(P_{ij}\\) the probability of moving from island \\(i\\) to island \\(j\\) and \\(N_i\\) the number of Kings on island \\(i\\). Let’s divide both sides by \\(N\\)\n\\[\nP_{ij}\\underbrace{P_{j}}_{N_j/N} = P_{ji}\\underbrace{P_{i}}_{N_j/N}\n\\]\nFigure 3 shows the detailed balance situation in a system with discrete states (such as kings). Note that the detailed balence is between two states \\(i\\) and \\(j\\) and that other states \\(k\\) are not taken into account.\n\n\n\n\n\n\nFigure 3: Illustration of the detailed balance condition in a Markov chain. The transition probabilities $P_{ij}$ and $P_{ji}$ between states $j$ and $i$ are shown, along with their respective stationary probabilities $P_i$ and $P_j$. The light gray dashed arrows indicate the presence of other states in the Markov chain, emphasizing that the detailed balance condition is specifically applied between states $i$ and $j$\n\n\n\nThe detailed balance condition the probability \\(P_{ij}P_j\\) (fraction of Kings/Particle) move from \\(j\\) to \\(i\\) is equal to the probability \\(P_{ji}P_i\\) (fraction of Kings/Particle) move from \\(i\\) to \\(j\\).\n\n🎛️Controling \\(P_{ij}\\)\nLet’s choose \\(P_{ij}\\) in our favor, so that in equilibrium the distribution \\(P_i\\) is the distribution we want. We can do this by proposing a move from \\(i\\) to \\(j\\) with a probability \\(T_{ij}\\) and then accept the move with an acceptance probability \\(A_{ij}\\). The cool idea that physicists had in the 1950’s is to choose the acceptence probability \\(A_{ij}\\) as:\n\nMetropolis-Hastings acceptance probability\n\\[\nA_{ij} = \\min (1, \\frac{T_{ji}P_i}{T_{ij} P_j})\n\\]\nWhy is detailed balance fullfilled?\n\nIf we know move a particle according to the Metropolis Hastings Acceptance rate. We start in state \\(\\theta_0\\) and propose a move to \\(\\theta^*\\) with \\(T(\\theta^*|\\theta_0)\\). We accept the new state with a probability according to the MH Acceptance probability. If we repeat we get the following chain of moves.\n\nInitialize: Start with an initial value \\(\\theta_0\\).\nPropose: new state \\(\\theta^*\\) from old state \\(\\theta_t\\) with a proposal distribution \\(T(\\theta^* | \\theta_t)\\).\nCalculate Acceptance Probability: Compute the acceptance probability \\[\n  A  = \\min \\left(1, \\frac{T(\\theta_t | \\theta^*) p(\\theta^*)}{T(\\theta^* | \\theta_t) p(\\theta_t)}\\right)\n\\]\nAccept proposed state: With probability \\(A\\), set \\(\\theta_{t+1} = \\theta^*\\). Otherwise, continue with old state, \\(\\theta_{t+1} = \\theta_t\\).\nIterate: Repeat steps 2-4 for a large number of iterations to ensure convergence to the stationary distribution.\n\n\n\nA short note on the continuous case\nSo far we have discussed the Metropolis-Hastings algorithm in the context of discrete states. However, the Metropolis-Hastings algorithm can be extended to continuous states, easily. There might be pitfalls when changing to the continuous case, don’t just replace the probabilities \\(P_i\\) with densities \\(p(\\theta)\\)! However here the Metropolis-Hastings algorithm is the same as in the discrete case. Just exchange the probabilities with densities.\n\n\n\n\n\n\nNote\n\n\n\nFor the acceptance rate \\(p(\\theta)\\) is needed only up to a constant factor, this makes it ideal for Bayesian inference, where we just need \\(p(\\theta|D) \\propto p(D|\\theta)p(\\theta)\\).\n\n\nThis process ensures that the Markov chain will converge to the target distribution \\(p(\\theta)\\), allowing us to approximate the distribution through the samples obtained from the chain. Let’s give it a try, we assume symetric proposal distribution \\(T(\\theta^*|\\theta) = T(\\theta|\\theta^*)\\) and we want to sample from the \\(p(\\theta) \\propto Exp(\\lambda = 1/10)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n📝 Exercise (Simple MCMC Algorithm)\n\n\n\n\n\n\n📝 Exercise (Simple MCMC Algorithm)\n\n\n\nPlay around with the code above.\n\nWhat do you observe if you change the standard deviation of the proposal distribution?\nChange the target distribution to \\(p(\\theta) \\propto \\exp(-\\theta^2/2)\\)."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#the-hairy-caterpillar",
    "href": "lecture_notes/MCMC/MCMC.html#the-hairy-caterpillar",
    "title": "Lecture Notes on MCMC",
    "section": "🐛The hairy caterpillar",
    "text": "🐛The hairy caterpillar\nThere can be several problems when using the Metropolis-Hastings algorithm.\n\nSlow mixing: The Markov chain takes a long time to converge to the stationary distribution.\nRandom walk behavior: Too small steps in the proposal distribution can lead to a behavior, where the chain moves slowly through the parameter space and many samples are correlated.\nTrap in local minima: The chain gets trapped in a local minimum and does not explore the parameter space properly\n\n\n🛠️ Tools to inspect the chain\nFirst look at the trace of the chain(s), this is called traceplot. Simply plot the parameter values against the steps. Such a traceplot has been shown in Figure 1. We clearly see that the 2 chains have converged to the same region (stationary distribution). Another traceplot is shown in Figure 4.\n\n\n\n\n\n\nFigure 4: Traceplot of the parameter sigma inagainst the steps. While the Metropolis Algorithm provides valid samples, the samples are highly correlated compared to the more advanced Stan algorithm\n\n\n\nThis traceplot compares the parameter \\(a\\) sampled with Stan (a more advanced MCMC algorithm) and the Metropolis-Hastings algorithm. We see that the Metropolis-Hastings algorithm moves slowly through the parameter space. In contrast, the Stan samples are less correlated and move more faster through the complete parameter space. This fast moving through the parameter space is called “efficient mixing” or a “hairy caterpillar”. The effect of this “hairy caterpillar” is that the samples are less correlated and hence provide more information about the posterior distribution of the parameter. This can be quantified by the effective sample size (ESS), which is the number of independent samples that provide the same information as the correlated samples.\nFinally, in the case of bimodal distributions, MCMC algorithms can get stuck in one of the modes. This is shown in Figure 5 . Some chains of have converged to the one mode and it takes a long time to switch to the other mode. While the true bimodal distribution would be sampled for infinite times it practically takes a long time to sample from both modes and the respecitve modes are not sampled equally.\n\n\n\n\n\n\nFigure 5: Trace Plot of Samples from a Bimodal Distribution, all chains sample the repective mode quite successfully but stay too long in a mode before jumping to the next.\n\n\n\nYou can play around a bit with the overlap of the two modes and the standard deviation of the proposal distribution in https://oduerr.github.io/anim/mcmc_mh.html"
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#continious-case-details",
    "href": "lecture_notes/MCMC/MCMC.html#continious-case-details",
    "title": "Lecture Notes on MCMC",
    "section": "🤔Continious case details*",
    "text": "🤔Continious case details*\n*This is and advanced topic, which might be well skipped in first reading.\nLet’s look at the continous case in more detail. We derived the Metropolis-Hastings algorithm for the discrete case, where we have probabilities and not densities. The MH acceptance probability is given by\n\\[\nA_{ij} = \\min (1, \\frac{T_{ji}P_i}{T_{ij} P_j})\n\\]\nThe probability \\(P_i\\) for the state \\(\\theta_i\\) is replaced by the density \\(p(\\theta_i)\\) times the infinitisimal volume element \\(d\\theta_i\\). The transition probability \\(T_{ji}\\) that starting in \\(i\\) we move to \\(j\\) is replaced by the proposal density \\(T(\\theta_j|\\theta_i)\\) times the infinitisimal volume element \\(d\\theta_j\\). Note we have a probability in the target volumne \\(d\\theta_j\\) and not in the “from” volume \\(d\\theta_i\\). So altogether we get\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{d\\theta_j T(\\theta_j|\\theta_i) p(\\theta_i) d\\theta_i}{d\\theta_i T(\\theta_i|\\theta_j) p(\\theta_j) d\\theta_j}\\right)\n\\] Note that the volume elements \\(d\\theta_i\\) and \\(d\\theta_j\\) cancel out. So we get\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{T(\\theta_j|\\theta_i) p(\\theta_i)}{T(\\theta_i|\\theta_j) p(\\theta_j) }\\right)\n\\tag{1}\\] Which is the same as in the discrete case! However, there might be pitfalls when changing to the continous case, don’t just replace the probabilities \\(P_i\\) with densities \\(p(\\theta)\\)! This is for example the case in the following.\n\n⚠️Sampling in a different space\nThere are several reasons, to sample in a different space, then the space where the target distribution \\(p(\\theta)\\) is defined. It might be hard to sample in \\(\\theta\\), but it’s easy to sample in another space \\(x\\). For example in \\(\\theta\\) the probability landscape might have very narrow regions, but in \\(x\\) it’s much nicer behaved. We will encounter this in the famous Neal’s funnel example later. Another reason is that \\(\\theta\\) is in a restricted space. We will use the following example in spherical coordinates to illustrate this. Suppose we have a problem where we have a good idea that the radius \\(r\\) is around 1, but we have no idea about the angle \\(\\varphi\\). We can express this in the following prior\n\\[\np(\\theta) = p(r,\\varphi) = p(r)p(\\varphi) = N(r|1,0.1) \\cdot U(\\varphi|0,2\\pi)\n\\] Assume, we have no data, then the Target Distribution is then given by \\(p(r,\\varphi) \\propto N(r|1,0.1)\\). Note that in this space, we have restrictions on the parameters \\(r &gt; 0\\) and \\(0 \\le \\varphi &lt; 2\\pi\\). We have to design the proposal density \\(T(\\theta^*|\\theta)\\) such that these restrictions are intact. While this is possible in the Metropolis-Hastings algorithm, it’s really hard to enforce this in more advanced algorithms like Hamiltonian Monte Carlo, where we need an unrestricted space. To solve this we transform the problem in a difference space \\(x\\), where we have no restrictions. The acceptance probability is then Equation 1 with \\(\\theta\\) replaced by \\(x\\).\n\\[\nA(x_i \\leftarrow x_j) = \\min \\left(1, \\frac{T(x_j|x_i) p(x_i)}{T(x_i|x_j) p(x_j) }\\right)\n\\tag{2}\\]\nSo what is missing is \\(p(x)\\). Be careful and consider probabilities:\n\\[\n  p(x) dx = p(\\theta) d\\theta \\quad \\Rightarrow \\quad p(x) = p(\\theta) \\left| \\frac{d \\theta}{d x} \\right|\n\\] With the Jacobian Determinant \\(J = \\left| \\frac{d\\theta}{dx} \\right|\\), e.g. Equation 1 becomes:\n\\[\nA(\\theta_i \\leftarrow \\theta_j) = \\min \\left(1, \\frac{T(\\theta_j|\\theta_i) p(\\theta_i)}{T(\\theta_i|\\theta_j) p(\\theta_j)} \\right) = \\min \\left(1, \\frac{T(x_j|x_i) p(x_i) J_i}{T(x_i|x_j) p(x_j) J_j} \\right)\n\\]\nWe have \\(x_1 = r \\cos(\\varphi)\\) and \\(x_2 = r \\sin(\\varphi)\\), so calculation of \\(\\frac{d x}{d \\theta}\\) would be simple, so we calculate the inverse of the Jacobian Determinant: \\[\nJ^{-1} =\n\\left|\n\\begin{pmatrix}\n\\frac{\\partial x_1}{\\partial r} & \\frac{\\partial x_1}{\\partial \\varphi} \\\\\n\\frac{\\partial x_2}{\\partial r} & \\frac{\\partial x_2}{\\partial \\varphi}\n\\end{pmatrix}\n\\right|\n=\n\\left|\n\\begin{pmatrix}\n\\cos(\\varphi) & -r \\sin(\\varphi) \\\\\n\\sin(\\varphi) & r \\cos(\\varphi)\n\\end{pmatrix}\n\\right| = r (\\cos^2(\\varphi) + \\sin^2(\\varphi)) = r\n\\] The Jacobian Determinant is then \\(J = 1/r\\). We can now sample in the unrestricted space \\(x_1\\) and \\(x_2\\) and apply the Jacobian Determinant. The following code snippet shows how to sample from the target distribution in unrestricted space \\(x_1\\) and \\(x_2\\) and how to correctly apply the Jacobian Determinant.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lecture_notes/MCMC/MCMC.html#footnotes",
    "href": "lecture_notes/MCMC/MCMC.html#footnotes",
    "title": "Lecture Notes on MCMC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell actually its a density plot plot(density(.)) and not hist(.)↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Internal Notes for the Author\n\nHow to add a new page\nPut the quarto-file in website directory. Make sure that you have commented out #self-contained: true in the qmd file, otherwise file gets to large.\n\nRender and move to docs folder\n\nquarto render _quarto.yml\n\nCheck in the changes in the docs folder"
  }
]